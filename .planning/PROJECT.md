# CineCut AI

## What This Is

A Python 3.10+ CLI tool that performs AI-driven narrative extraction from full-length feature films to generate a stylized ~2-minute trailer based on a user-defined "Vibe Profile." The system runs a multi-stage pipeline: analysis proxy creation → hybrid keyframe extraction → subtitle parsing → LLaVA multimodal inference (cached via msgpack) → structural analysis (Mistral 7B, BEGIN_T/ESCALATION_T/CLIMAX_T) → zone-based clip assignment and non-linear ordering → BPM-synced assembly with silence segmentation → FFmpeg conform with music bed, synthesized SFX, protagonist VO, and four-stem audio mix. No cloud dependencies — everything runs locally on a Quadro K6000 with CUDA 11.4. Shipped v2.0 with 5,644 LOC Python, 207 unit tests, and all 50 requirements complete.

## Core Value

Given a feature film and its subtitle file, produce a narratively coherent, vibe-styled trailer that a human editor would be proud to show — technically clean output with a real beginning, escalation, and climax arc.

## Requirements

### Validated

- ✓ Three-tier pipeline: 420p analysis proxy → multimodal inference → high-bitrate conform — v1.0
- ✓ 18 vibe profiles (Action, Adventure, Animation, Comedy, Crime, Documentary, Drama, Family, Fantasy, History, Horror, Music, Mystery, Romance, Sci-Fi, Thriller, War, Western) — v1.0
- ✓ Each vibe has an Edit Profile (avg cut length per act, audio LUFS target, transition style, LUT spec) — v1.0
- ✓ LLaVA vision model integration via llama-server HTTP mode for persistent inference — v1.0
- ✓ SRT/ASS subtitle file as primary narrative signal (always provided by user) — v1.0
- ✓ Narrative beat extraction: Inciting Incident, Climax Beats, Money Shots (7 beat types, 8-signal scorer) — v1.0
- ✓ TRAILER_MANIFEST.json generated by AI with all clip decisions, reasoning, and per-clip treatment — v1.0
- ✓ `--review` flag pauses pipeline after manifest generation for human inspection/edit — v1.0
- ✓ Default mode: fully automatic (manifest → conform without pause) — v1.0
- ✓ LUT sourcing and creation for all 18 vibes (.cube files, programmatically generated via NumPy) — v1.0
- ✓ Frame-accurate FFmpeg seeking (-ss before -i) — v1.0
- ✓ Audio normalization with LUFS targeting per vibe (two-pass loudnorm; single-pass for clips < 3s) — v1.0
- ✓ LUT application via FFmpeg lut3d filter based on vibe selection — v1.0
- ✓ CLI interface: `cinecut <video> --subtitle <srt> --vibe <name> [--review]` — v1.0
- ✓ SceneDescription persistence (msgpack cache) — pipeline resume skips 30-60 min LLaVA re-inference — v2.0
- ✓ Two-stage LLM pipeline: text structural analysis (Mistral 7B) + scene-zone matching (sentence-transformers) — v2.0
- ✓ Non-linear scene ordering by semantic zone (BEGINNING → ESCALATION → CLIMAX), not film chronology — v2.0
- ✓ BPM-driven edit pacing grid with octave correction and per-vibe defaults — v2.0
- ✓ Music bed per vibe (Jamendo CC-licensed, auto-downloaded, permanently cached) with dynamic ducking — v2.0
- ✓ Transition SFX synthesized via FFmpeg aevalsrc — no external SFX files required — v2.0
- ✓ Silence/breathing room segment (3-5s) at Act 2→3 boundary as explicit editorial tool — v2.0
- ✓ Protagonist VO extracted from film audio at dialogue timestamps, placed in Acts 1-2 — v2.0
- ✓ Four-stem audio mix: film audio + music + SFX + VO with sidechaincompress ducking — v2.0

### Active

(None — planning next milestone)

### Out of Scope

- Cloud inference or remote API calls — local-only by constraint
- Subtitle generation / speech-to-text — user always provides subtitle file
- Mobile or web interface — CLI only
- Real-time preview during editing — manifest-based workflow only
- Multi-GPU or distributed processing — single Quadro K6000
- Ollama or Python-native LLM loading — llama-server is the pre-configured engine; deviating risks CUDA 11.4 instability
- VO quality scoring beyond dialogue-line-count heuristic — deferred to v3.0
- Per-scene SFX intensity calibration — fixed tiers adequate for v2.0; deferred to v3.0
- Music archive management UI — not yet needed; deferred to v3.0

## Context

**Shipped v2.0 on 2026-02-28.** 5,644 Python LOC across 36 src/ files. 60 commits over 1 day. 207 unit tests, 0 failures.

**v1.0 shipped 2026-02-27.** 4,822 Python LOC, 119 tests, 78 commits over 2 days.

**Tech stack:** Python 3.10+, Typer + Rich (CLI), Pydantic (manifest schema), pysubs2 (SRT/ASS), OpenCV headless (keyframes, face detection), NumPy (LUT generation), requests (llama-server HTTP + Jamendo API), librosa + soundfile (BPM detection), sentence-transformers (zone matching), FFmpeg (proxy + conform + lavfi + audio mix).

**Hardware:** Nvidia Quadro K6000 (12GB VRAM) | Driver 470.256.02 | CUDA 11.4

**Inference:** llama-server HTTP mode (persistent, avoids per-frame model reload). GPU_LOCK enforces sequential GPU use — LlavaEngine (port 8089) and TextEngine (port 8090) never run concurrently; wait_for_vram() polling between model swaps. mmproj binary patch (42 bytes, clip.projector_type = "mlp") required for llama.cpp 8156 compatibility with mys/ggml_llava-v1.5-7b.

**v2.0 resolved:** SceneDescription (inference results) now persisted to `<work_dir>/<stem>.scenedesc.msgpack` — resume after failure skips Stage 4 inference entirely.

## Constraints

- **GPU/CUDA:** Quadro K6000, CUDA 11.4 — no newer CUDA features; llama-cli/llama-server must be CUDA 11.4 compatible
- **Inference:** llama-server is the only inference backend; no Ollama, no Python-native model loading
- **VRAM:** 12GB hard ceiling for all concurrent processes
- **Subtitles:** Always SRT or ASS format — system does not handle subtitle-less input
- **Python version:** 3.10+ (use match/case, modern type hints)
- **Dependencies:** FFmpeg must be available on PATH; no bundled binaries

## Key Decisions

| Decision | Rationale | Outcome |
|----------|-----------|---------|
| LLaVA vision model via llama-server (not text-only) | Keyframes analyzed as images — richer scene description than subtitle-only | ✓ Good — structured JSON scene descriptions validated; mmproj binary patch required for llama.cpp 8156 |
| SRT/ASS as always-present input | Simplifies narrative extraction; user workflow always has subtitles available | ✓ Good — dual subtitle path (SRT/ASS) works cleanly; pysubs2 .plaintext strips ASS override tags |
| JSON manifest as pipeline intermediary | Enables human override before expensive high-res conform; also a debugging artifact | ✓ Good — --review workflow validates; manifest is load/edit/re-conform friendly |
| `--review` flag for manifest inspection (not default pause) | Default auto mode is fast path; `--review` is explicit opt-in for editorial control | ✓ Good — clean CLI UX; users can always resume from --manifest flag |
| LUTs programmatically generated via NumPy | 18 vibe-specific .cube files not available; procedural generation provides full coverage | ✓ Good — identity-based NumPy transforms + hue/contrast/saturation shifts produce valid 33^3 LUTs |
| 420p analysis proxy | Hardware-constrained — reduces FFmpeg memory pressure during inference pipeline | ✓ Good — confirmed reduces VRAM contention; proxy deleted after inference stage |
| stdlib dataclasses for ingestion-layer models | Pydantic validation overhead not needed at ingestion | ✓ Good — Pydantic reserved for manifest schema; boundary is clean |
| hatchling over setuptools | Handles src/ layout automatically | ✓ Good — no [tool.setuptools] sections needed |
| Short clips < 3s use volume=0dB (not two-pass loudnorm) | loudnorm unstable on sub-3s audio (Act 3 montage clips) | ✓ Good — prevents FFmpeg instability; acceptable for short clips |
| Atomic checkpoint via tempfile + os.replace() | POSIX-atomic; power-loss-safe | ✓ Good — same-mount temp file guarantees atomic rename on Linux |
| SceneDescription persisted to msgpack (v2.0) | Eliminates 30-60min re-inference penalty on crash resume | ✓ Good — invalidates on mtime/size change; cascade checkpoint reset prevents stale stage data |
| title_card and button as pre-encoded MP4 files (FFmpeg lavfi) | ClipEntry objects would extract first 5s of film as fake segments | ✓ Good — lavfi black segment is correct architecture |
| TextEngine on port 8090, LlavaEngine on port 8089 | Never run concurrently — wait_for_vram() polling required between model swaps | ✓ Good — prevents VRAM exhaustion on K6000; clean separation of inference backends |
| CPU sentence-transformers for zone matching (all-MiniLM-L6-v2) | GPU sentence-transformers incompatible with CUDA 11.4 stack | ✓ Good — CPU inference fast enough for clip-count workloads; util patched at module level for test isolation |
| amix normalize=0 mandatory throughout | normalize=1 destroys sidechain ducking ratios | ✓ Good — stem-level loudnorm at −16 LUFS before mixing preserves ducking ratios correctly |
| Jamendo API v3 with audiodownload_allowed=True filter | April 2022 API change — tracks without this flag return 404 on download | ✓ Good — filter required for reliable track selection |
| soundfile>=0.12.1 pinned for MP3 support | librosa.load() on .mp3 raises LibsndfileError without bundled libsndfile 1.1.0+ | ✓ Good — pinned version prevents regression |
| NarrativeZone as (str, Enum) | Pydantic v2 serializes plain string ("BEGINNING") not dict; backward compat via Optional default=None | ✓ Good — old v1.0 manifests load without ValidationError |

---
*Last updated: 2026-02-28 after v2.0 milestone*
