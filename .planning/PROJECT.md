# CineCut AI

## What This Is

A Python 3.10+ CLI tool that performs AI-driven narrative extraction from full-length feature films to generate a stylized ~2-minute trailer based on a user-defined "Vibe Profile." The system runs a 7-stage pipeline: analysis proxy creation → hybrid keyframe extraction → subtitle parsing → LLaVA/llama-server multimodal inference → narrative beat classification and manifest generation → 3-act assembly with pacing curves → high-bitrate FFmpeg conform against the original source file. No cloud dependencies — everything runs locally on a Quadro K6000 with CUDA 11.4. Shipped v1.0 with 4,822 LOC Python, 119 unit tests, and all 24 v1 requirements complete.

## Core Value

Given a feature film and its subtitle file, produce a narratively coherent, vibe-styled trailer that a human editor would be proud to show — technically clean output with a real beginning, escalation, and climax arc.

## Requirements

### Validated

- ✓ Three-tier pipeline: 420p analysis proxy → multimodal inference → high-bitrate conform — v1.0
- ✓ 18 vibe profiles (Action, Adventure, Animation, Comedy, Crime, Documentary, Drama, Family, Fantasy, History, Horror, Music, Mystery, Romance, Sci-Fi, Thriller, War, Western) — v1.0
- ✓ Each vibe has an Edit Profile (avg cut length per act, audio LUFS target, transition style, LUT spec) — v1.0
- ✓ LLaVA vision model integration via llama-server HTTP mode for persistent inference — v1.0
- ✓ SRT/ASS subtitle file as primary narrative signal (always provided by user) — v1.0
- ✓ Narrative beat extraction: Inciting Incident, Climax Beats, Money Shots (7 beat types, 8-signal scorer) — v1.0
- ✓ TRAILER_MANIFEST.json generated by AI with all clip decisions, reasoning, and per-clip treatment — v1.0
- ✓ `--review` flag pauses pipeline after manifest generation for human inspection/edit — v1.0
- ✓ Default mode: fully automatic (manifest → conform without pause) — v1.0
- ✓ LUT sourcing and creation for all 18 vibes (.cube files, programmatically generated via NumPy) — v1.0
- ✓ Frame-accurate FFmpeg seeking (-ss before -i) — v1.0
- ✓ Audio normalization with LUFS targeting per vibe (two-pass loudnorm; single-pass for clips < 3s) — v1.0
- ✓ LUT application via FFmpeg lut3d filter based on vibe selection — v1.0
- ✓ CLI interface: `cinecut <video> --subtitle <srt> --vibe <name> [--review]` — v1.0

### Active

- [ ] Stage-based checkpoint resume loses inference results on resume — SceneDescription persistence deferred to v2
- [ ] No subtitle-less input mode (graceful degradation to visual-only) — tracked as INPUT-01

### Out of Scope

- Cloud inference or remote API calls — local-only by constraint
- Subtitle generation / speech-to-text — user always provides subtitle file
- Mobile or web interface — CLI only
- Real-time preview during editing — manifest-based workflow only
- Multi-GPU or distributed processing — single Quadro K6000
- Ollama or Python-native LLM loading — llama-server is the pre-configured engine; deviating risks CUDA 11.4 instability
- Offline mode with cached models — llama-server persistent process covers this

## Context

**Shipped v1.0 on 2026-02-27.** 4,822 Python LOC across 95 files. 78 commits over 2 days (2026-02-26 → 2026-02-27). 119 unit tests, 0 failures.

**Tech stack:** Python 3.10+, Typer + Rich (CLI), Pydantic (manifest schema), pysubs2 (SRT/ASS), OpenCV headless (keyframes, face detection), NumPy (LUT generation), requests (llama-server HTTP), FFmpeg (proxy + conform + lavfi).

**Hardware:** Nvidia Quadro K6000 (12GB VRAM) | Driver 470.256.02 | CUDA 11.4

**Inference:** llama-server HTTP mode (persistent, avoids per-frame model reload). GPU_LOCK enforces sequential GPU use — llama-server and FFmpeg never run concurrently. mmproj binary patch (42 bytes, clip.projector_type = "mlp") required for llama.cpp 8156 compatibility with mys/ggml_llava-v1.5-7b.

**Known limitations from v1.0:** SceneDescription (inference results) are not persisted — resume after failure re-runs inference from scratch for Stage 4.

## Constraints

- **GPU/CUDA:** Quadro K6000, CUDA 11.4 — no newer CUDA features; llama-cli/llama-server must be CUDA 11.4 compatible
- **Inference:** llama-server is the only inference backend; no Ollama, no Python-native model loading
- **VRAM:** 12GB hard ceiling for all concurrent processes
- **Subtitles:** Always SRT or ASS format — system does not handle subtitle-less input
- **Python version:** 3.10+ (use match/case, modern type hints)
- **Dependencies:** FFmpeg must be available on PATH; no bundled binaries

## Key Decisions

| Decision | Rationale | Outcome |
|----------|-----------|---------|
| LLaVA vision model via llama-server (not text-only) | Keyframes analyzed as images — richer scene description than subtitle-only | ✓ Good — structured JSON scene descriptions validated; mmproj binary patch required for llama.cpp 8156 |
| SRT/ASS as always-present input | Simplifies narrative extraction; user workflow always has subtitles available | ✓ Good — dual subtitle path (SRT/ASS) works cleanly; pysubs2 .plaintext strips ASS override tags |
| JSON manifest as pipeline intermediary | Enables human override before expensive high-res conform; also a debugging artifact | ✓ Good — --review workflow validates; manifest is load/edit/re-conform friendly |
| `--review` flag for manifest inspection (not default pause) | Default auto mode is fast path; `--review` is explicit opt-in for editorial control | ✓ Good — clean CLI UX; users can always resume from --manifest flag |
| LUTs programmatically generated via NumPy | 18 vibe-specific .cube files not available; procedural generation provides full coverage | ✓ Good — identity-based NumPy transforms + hue/contrast/saturation shifts produce valid 33^3 LUTs |
| 420p analysis proxy | Hardware-constrained — reduces FFmpeg memory pressure during inference pipeline | ✓ Good — confirmed reduces VRAM contention; proxy deleted after inference stage |
| stdlib dataclasses for ingestion-layer models | Pydantic validation overhead not needed at ingestion | ✓ Good — Pydantic reserved for manifest schema; boundary is clean |
| hatchling over setuptools | Handles src/ layout automatically | ✓ Good — no [tool.setuptools] sections needed |
| Short clips < 3s use volume=0dB (not two-pass loudnorm) | loudnorm unstable on sub-3s audio (Act 3 montage clips) | ✓ Good — prevents FFmpeg instability; acceptable for short clips |
| Atomic checkpoint via tempfile + os.replace() | POSIX-atomic; power-loss-safe | ✓ Good — same-mount temp file guarantees atomic rename on Linux |
| SceneDescription not persisted (v1.0) | Inference persistence adds schema/IO complexity; v1.0 tight scope | ⚠️ Revisit — resume after failure re-runs 30-60min inference stage; track as v2 priority |
| title_card and button as pre-encoded MP4 files (FFmpeg lavfi) | ClipEntry objects would extract first 5s of film as fake segments | ✓ Good — lavfi black segment is correct architecture |

---
*Last updated: 2026-02-27 after v1.0 milestone*
