---
phase: 06-inference-persistence
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/cinecut/inference/cache.py
  - src/cinecut/checkpoint.py
  - src/cinecut/cli.py
  - tests/test_cache.py
  - pyproject.toml
autonomous: true
requirements:
  - IINF-01
  - IINF-02

must_haves:
  truths:
    - "Running cinecut a second time on the same film skips Stage 4 LLaVA inference — Rich output prints 'Cache hit' and lists the number of SceneDescriptions loaded"
    - "A completed pipeline run produces a .scenedesc.msgpack file inside the work directory alongside pipeline_checkpoint.json"
    - "Modifying the source film file (or replacing it) causes the cache to be invalidated — inference re-runs and cache is rewritten"
    - "A corrupt or missing cache file is treated as a cache miss — inference runs normally with no error raised"
    - "After a mtime/size-triggered cache invalidation, the narrative and assembly stages in the checkpoint are also cleared — preventing Stage 5 from running against stale keyframes"
  artifacts:
    - path: "src/cinecut/inference/cache.py"
      provides: "save_cache(), load_cache(), _cache_path() — msgpack-based SceneDescription persistence"
      exports: ["save_cache", "load_cache"]
    - path: "src/cinecut/checkpoint.py"
      provides: "PipelineCheckpoint with cache_hit: Optional[bool] field"
      contains: "cache_hit"
    - path: "src/cinecut/cli.py"
      provides: "Stage 4 guard: load_cache() before run_inference_stage(); save_cache() after inference"
      contains: "load_cache"
    - path: "tests/test_cache.py"
      provides: "Unit tests for save_cache, load_cache, invalidation, and corrupt-file behaviour"
      min_lines: 60
  key_links:
    - from: "src/cinecut/cli.py Stage 4"
      to: "src/cinecut/inference/cache.py:load_cache"
      via: "import and call at Stage 4 entry point"
      pattern: "load_cache\\("
    - from: "src/cinecut/inference/cache.py:save_cache"
      to: "work_dir/<stem>.scenedesc.msgpack"
      via: "tempfile.mkstemp + os.replace atomic write"
      pattern: "os\\.replace"
    - from: "src/cinecut/inference/cache.py:load_cache"
      to: "os.stat mtime + size"
      via: "source_file.stat().st_mtime and .st_size comparison"
      pattern: "st_mtime"
---

<objective>
Add a msgpack-based SceneDescription cache to eliminate the 30-60 minute LLaVA re-inference penalty when resuming a failed or interrupted pipeline run.

Purpose: A crash after Stage 4 currently forces re-inference from scratch — the single most expensive operation in the pipeline. With a cache, every resume after the first full run costs zero GPU time for Stage 4.
Output: inference/cache.py (new), checkpoint.py (1 field added), cli.py (Stage 4 guard added), tests/test_cache.py (new).
</objective>

<execution_context>
@/home/adamh/.claude/get-shit-done/workflows/execute-plan.md
@/home/adamh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-inference-persistence/06-RESEARCH.md

@src/cinecut/inference/models.py
@src/cinecut/models.py
@src/cinecut/checkpoint.py
@src/cinecut/cli.py
</context>

<interfaces>
<!-- Key types the executor needs. Extracted from codebase. -->

From src/cinecut/inference/models.py:
```python
@dataclass
class SceneDescription:
    visual_content: str
    mood: str
    action: str
    setting: str
```

From src/cinecut/models.py:
```python
@dataclass
class KeyframeRecord:
    timestamp_s: float   # PTS seconds in the proxy
    frame_path: str      # Absolute path to the JPEG file
    source: str          # "subtitle_midpoint" | "scene_change" | "interval_fallback"
```

From src/cinecut/checkpoint.py:
```python
@dataclass
class PipelineCheckpoint:
    source_file: str
    vibe: str
    stages_complete: list[str] = field(default_factory=list)
    proxy_path: Optional[str] = None
    keyframe_count: Optional[int] = None
    dialogue_event_count: Optional[int] = None
    inference_complete: Optional[bool] = None
    manifest_path: Optional[str] = None
    assembly_manifest_path: Optional[str] = None

    def is_stage_complete(self, stage: str) -> bool: ...
    def mark_stage_complete(self, stage: str) -> None: ...

def save_checkpoint(checkpoint: PipelineCheckpoint, work_dir: Path) -> None: ...
def load_checkpoint(work_dir: Path) -> Optional[PipelineCheckpoint]: ...
```

From src/cinecut/cli.py (Stage 4 region — lines 305-338):
```python
# --- Stage 4/7: LLaVA Inference (INFR-02) ---
# TODO: inference resume requires persisting SceneDescription results; deferred to v2
console.print(f"[bold]Stage 4/{TOTAL_STAGES}:[/bold] Running LLaVA inference on keyframes...")
inference_results = []

with Progress(...) as progress:
    infer_task = progress.add_task("Describing frames...", total=len(keyframe_records))
    def _progress_callback(current: int, total: int) -> None:
        progress.update(infer_task, completed=current)
    inference_results = run_inference_stage(
        keyframe_records, model, mmproj,
        progress_callback=_progress_callback,
    )

skipped = sum(1 for _, desc in inference_results if desc is None)
ckpt.inference_complete = True
save_checkpoint(ckpt, work_dir)
console.print(f"[green]Inference complete:[/] {len(inference_results)} frames processed, {skipped} skipped\n")
```

work_dir naming convention:
```python
def _setup_work_dir(source: Path) -> Path:
    work_dir = source.parent / f"{source.stem}_cinecut_work"
    work_dir.mkdir(exist_ok=True)
    (work_dir / "keyframes").mkdir(exist_ok=True)
    return work_dir
```
</interfaces>

<tasks>

<task type="auto">
  <name>Task 1: Create inference/cache.py with msgpack persistence and mtime invalidation</name>
  <files>src/cinecut/inference/cache.py</files>
  <action>
Create `src/cinecut/inference/cache.py` implementing three public functions: `_cache_path()`, `save_cache()`, `load_cache()`.

Cache file location: `work_dir / f"{source_file.stem}.scenedesc.msgpack"` — stored inside the work directory so deleting the work dir also clears the cache. Do NOT use `~/.cinecut/` or any global location.

`save_cache(results, source_file, work_dir)`:
- `results` type: `list[tuple[KeyframeRecord, SceneDescription | None]]`
- Build payload dict: `{"metadata": {"source_file": str(source_file), "mtime": stat.st_mtime, "size": stat.st_size}, "results": [{"record": asdict(record), "description": asdict(desc) if desc is not None else None} for record, desc in results]}`
- Use `msgpack.packb(payload, use_bin_type=True)`
- Atomic write: `tempfile.mkstemp(dir=work_dir, suffix=".cache.tmp")` + `os.write` + `os.fsync` + `os.close` + `os.replace` — mirror the exact pattern from `checkpoint.py:save_checkpoint()`
- Returns the `Path` to the written cache file

`load_cache(source_file, work_dir)`:
- Returns `list[tuple[KeyframeRecord, SceneDescription | None]] | None`
- Returns `None` if cache file doesn't exist
- Returns `None` on any `Exception` during `msgpack.unpackb` — corrupt = miss, never propagate
- Always use `msgpack.unpackb(data, raw=False, strict_map_key=False)` — mixing raw modes causes `KeyError: b'metadata'`
- Read `meta["mtime"]` and `meta["size"]`, compare against `source_file.stat().st_mtime` and `source_file.stat().st_size` — if EITHER differs, return `None` (invalidated, not corrupt)
- Reconstruct: `KeyframeRecord(**item["record"])` and `SceneDescription(**item["description"]) if item["description"] is not None else None`
- Returns the full `list[tuple]` on a valid cache hit

Key pitfalls to avoid (from research):
- `asdict(None)` raises TypeError — guard with `if desc is not None`
- `strict_map_key=True` default can cause key issues — explicitly set `strict_map_key=False`
- Never raise from `load_cache()` on corrupt data — always `return None`

Add module docstring explaining the cache file format and invalidation strategy.

Install msgpack: add `"msgpack>=1.1.0"` to `pyproject.toml` `[project] dependencies` list, then run `pip install msgpack` in the project venv.
  </action>
  <verify>
```bash
cd /home/adamh/ai-video-trailer && python -c "from cinecut.inference.cache import save_cache, load_cache; print('import ok')"
```
  </verify>
  <done>Module imports cleanly; save_cache and load_cache are callable; msgpack listed in pyproject.toml dependencies.</done>
</task>

<task type="auto">
  <name>Task 2: Extend PipelineCheckpoint and wire Stage 4 guard in cli.py</name>
  <files>src/cinecut/checkpoint.py, src/cinecut/cli.py</files>
  <action>
**checkpoint.py:**
Add `cache_hit: Optional[bool] = None` as a new field on the `PipelineCheckpoint` dataclass (after `inference_complete`). This is observability-only — it tells the checkpoint file whether Stage 4 was served from cache. No other logic changes needed.

**cli.py:**
Replace the entire Stage 4 block (lines 305-338, including the TODO comment) with a cache-guarded version. The replacement must:

1. Import `load_cache` and `save_cache` from `cinecut.inference.cache` at the top of `cli.py` alongside the other `inference` imports.

2. At Stage 4 entry, attempt `load_cache(video, work_dir)` BEFORE starting the Progress context:

```python
# --- Stage 4/7: LLaVA Inference (INFR-01, IINF-01, IINF-02) ---
console.print(f"[bold]Stage 4/{TOTAL_STAGES}:[/bold] LLaVA inference...")
cached_results = load_cache(video, work_dir)

if cached_results is not None:
    # IINF-01: cache hit — skip inference entirely
    inference_results = cached_results
    ckpt.cache_hit = True
    cache_path = work_dir / f"{video.stem}.scenedesc.msgpack"
    console.print(
        f"[yellow]Cache hit:[/] Loaded {len(inference_results)} SceneDescriptions "
        f"from [dim]{cache_path.name}[/dim] — LLaVA inference skipped\n"
    )
else:
    # Cache miss or invalidated (IINF-02) — cascade reset then run inference
    # If source file changed (mtime/size differs), clear downstream stages
    # so Stage 5 (narrative) doesn't run against stale keyframes (Research Pitfall 5)
    for stale_stage in ("narrative", "assembly"):
        if stale_stage in ckpt.stages_complete:
            ckpt.stages_complete.remove(stale_stage)
            ckpt.manifest_path = None if stale_stage == "narrative" else ckpt.manifest_path
            ckpt.assembly_manifest_path = None if stale_stage == "assembly" else ckpt.assembly_manifest_path

    ckpt.cache_hit = False
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("{task.completed}/{task.total}"),
        TimeElapsedColumn(),
        console=console,
    ) as progress:
        infer_task = progress.add_task(
            "Describing frames...", total=len(keyframe_records)
        )

        def _progress_callback(current: int, total: int) -> None:
            progress.update(infer_task, completed=current)

        inference_results = run_inference_stage(
            keyframe_records,
            model,
            mmproj,
            progress_callback=_progress_callback,
        )

    save_cache(inference_results, video, work_dir)
    console.print(f"[green]Inference complete:[/] cache written\n")

skipped = sum(1 for _, desc in inference_results if desc is None)
ckpt.inference_complete = True
save_checkpoint(ckpt, work_dir)
```

3. The cascade reset (removing "narrative"/"assembly" from `stages_complete`) only runs on the `else` branch — when cache is missing or invalidated. It does NOT run on a cache hit. This prevents Stage 5 from running against stale keyframes after a source file change (Research Pitfall 5).

4. Do NOT add a `is_stage_complete("inference")` outer guard — Stage 4 was never guarded by checkpoint in v1.0 because results couldn't be persisted. The new cache IS the persistence mechanism — the guard is `load_cache() is not None`.
  </action>
  <verify>
```bash
cd /home/adamh/ai-video-trailer && python -c "from cinecut.checkpoint import PipelineCheckpoint; ckpt = PipelineCheckpoint(source_file='/tmp/x.mkv', vibe='action'); assert hasattr(ckpt, 'cache_hit'); print('checkpoint ok')" && python -c "from cinecut.cli import app; print('cli import ok')"
```
  </verify>
  <done>PipelineCheckpoint has cache_hit field; cli.py imports load_cache/save_cache without error; Stage 4 block no longer contains the TODO comment.</done>
</task>

<task type="auto">
  <name>Task 3: Write unit tests for cache module</name>
  <files>tests/test_cache.py</files>
  <action>
Create `tests/test_cache.py` covering all observable behaviours of `inference/cache.py`. Use `tmp_path` pytest fixture for all file I/O — no real model files needed, all tests are pure unit tests.

Required test cases (one `def test_*` per case):

1. **test_save_and_load_roundtrip** — Write a list of `(KeyframeRecord, SceneDescription)` tuples, load them back, assert equality of all fields. Use at least 3 records including one with `description=None` to cover the None-guard.

2. **test_cache_hit_returns_list** — `load_cache()` on a valid, matching cache file returns a non-None list of the same length as was saved.

3. **test_cache_miss_no_file** — `load_cache()` with no cache file present returns `None` (no exception).

4. **test_cache_invalidated_mtime_change** — Save a cache, then patch `source_file.stat()` to return a different `st_mtime` value. Assert `load_cache()` returns `None`.

5. **test_cache_invalidated_size_change** — Save a cache, then write extra bytes to the source file to change its real `st_size`. Assert `load_cache()` returns `None`.

6. **test_corrupt_cache_returns_none** — Write garbage bytes to the `.scenedesc.msgpack` file path. Assert `load_cache()` returns `None` (no exception propagated).

7. **test_cache_file_location** — After `save_cache()`, assert the returned path is `work_dir / f"{source_stem}.scenedesc.msgpack"` — NOT in `~/.cinecut/` or any other location.

8. **test_none_description_survives_roundtrip** — A result tuple `(record, None)` saves and loads back as `(record, None)` without raising `TypeError`.

Test file setup pattern (use for all tests):
```python
from pathlib import Path
from cinecut.inference.cache import save_cache, load_cache
from cinecut.inference.models import SceneDescription
from cinecut.models import KeyframeRecord

def make_record(ts: float, tmp_path: Path) -> KeyframeRecord:
    f = tmp_path / f"frame_{int(ts*1000):06d}.jpg"
    f.write_bytes(b"\xff\xd8\xff")  # minimal fake JPEG
    return KeyframeRecord(timestamp_s=ts, frame_path=str(f), source="subtitle_midpoint")

def make_desc() -> SceneDescription:
    return SceneDescription(visual_content="forest", mood="tense", action="running", setting="night")
```

For the mtime-change test, use `unittest.mock.patch.object` to mock `Path.stat` returning a `stat_result` with a different `st_mtime` value — or just copy the source file to a new path (change the source_file argument) after saving the cache. The simplest approach: save cache for `source_a`, then call `load_cache(source_b, work_dir)` where `source_b` is a different file (different mtime/size). The cache stores `source_a`'s mtime/size; `source_b`'s stat won't match → returns None.

Run tests with: `pytest tests/test_cache.py -v`
  </action>
  <verify>
```bash
cd /home/adamh/ai-video-trailer && pytest tests/test_cache.py -v 2>&1 | tail -20
```
  </verify>
  <done>All 8 test cases pass; no warnings about missing imports; pytest exits 0.</done>
</task>

</tasks>

<verification>
Full phase verification after all tasks complete:

1. Cache module exists and is importable:
```bash
cd /home/adamh/ai-video-trailer && python -c "from cinecut.inference.cache import save_cache, load_cache; print('ok')"
```

2. All cache unit tests pass:
```bash
cd /home/adamh/ai-video-trailer && pytest tests/test_cache.py -v
```

3. Existing test suite still passes (no regressions):
```bash
cd /home/adamh/ai-video-trailer && pytest tests/ -v --ignore=tests/test_inference.py -x 2>&1 | tail -20
```
(test_inference.py skipped — requires model files on disk)

4. Cache file produced by a real run lands in work_dir:
```bash
ls /tmp/*_cinecut_work/*.scenedesc.msgpack 2>/dev/null || echo "No real run yet — verify path logic via unit test_cache_file_location"
```

5. msgpack in dependencies:
```bash
cd /home/adamh/ai-video-trailer && grep "msgpack" pyproject.toml
```
</verification>

<success_criteria>
- `inference/cache.py` exists with `save_cache()` and `load_cache()` — both importable from `cinecut.inference.cache`
- `pyproject.toml` lists `msgpack>=1.1.0` in dependencies
- `PipelineCheckpoint` has `cache_hit: Optional[bool] = None` field
- `cli.py` Stage 4 block: `load_cache()` called before `run_inference_stage()`; `save_cache()` called after inference; "Cache hit" Rich message printed on hit
- Cascade reset removes "narrative" and "assembly" from `stages_complete` when cache is invalidated (not when hit)
- All 8 tests in `tests/test_cache.py` pass
- Existing test suite exits 0 (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/06-inference-persistence/06-01-SUMMARY.md` following the summary template at @/home/adamh/.claude/get-shit-done/templates/summary.md
</output>
```
