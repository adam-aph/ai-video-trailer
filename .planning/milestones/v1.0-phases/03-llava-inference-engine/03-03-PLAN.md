---
phase: 03-llava-inference-engine
plan: 03
type: execute
wave: 3
depends_on:
  - 03-02
files_modified:
  - src/cinecut/inference/engine.py
  - src/cinecut/cli.py
  - tests/test_inference.py
autonomous: false
requirements:
  - INFR-02
  - INFR-03
  - PIPE-05

must_haves:
  truths:
    - "describe_frame() submits a keyframe to LLaVA and returns a SceneDescription with all 4 fields"
    - "describe_frame() returns None on timeout, HTTP error, or malformed JSON — never raises globally"
    - "run_inference_stage() processes all KeyframeRecords sequentially, one frame at a time"
    - "CLI shows Rich progress bar during inference stage when invoked with --manifest flag"
    - "Unit tests for describe_frame pass using mocked requests.post"
    - "Integration test for server health passes when model files are present"
  artifacts:
    - path: "src/cinecut/inference/engine.py"
      provides: "describe_frame() method on LlavaEngine + run_inference_stage() function"
      exports: ["LlavaEngine.describe_frame", "run_inference_stage"]
    - path: "src/cinecut/cli.py"
      provides: "Inference stage wired into CLI pipeline with Rich progress"
      contains: "run_inference_stage"
    - path: "tests/test_inference.py"
      provides: "Passing unit tests for INFR-02 (describe_frame) using mock server"
      min_lines: 80
  key_links:
    - from: "src/cinecut/inference/engine.py"
      to: "llama-server /chat/completions"
      via: "requests.post with json_schema constraint"
      pattern: "json_schema.*SCENE_DESCRIPTION_SCHEMA"
    - from: "src/cinecut/inference/engine.py"
      to: "src/cinecut/inference/models.py"
      via: "validate_scene_description(json.loads(content))"
      pattern: "validate_scene_description"
    - from: "src/cinecut/cli.py"
      to: "src/cinecut/inference/engine.py"
      via: "run_inference_stage called in pipeline"
      pattern: "run_inference_stage"
---

<objective>
Add describe_frame() to LlavaEngine and wire the inference stage into the CLI. Complete the test scaffold by enabling and implementing the two INFR-02 unit tests.

Purpose: This plan delivers the core inference loop (INFR-02) and closes the loop with CLI integration so the full pipeline is visible to the user. The human-verify checkpoint confirms the engine works end-to-end with the real LLaVA model.
Output: LlavaEngine.describe_frame(), run_inference_stage(), CLI inference progress, passing unit tests, and human-verified integration.
</objective>

<execution_context>
@/home/adamh/.claude/get-shit-done/workflows/execute-plan.md
@/home/adamh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-llava-inference-engine/03-RESEARCH.md
@.planning/phases/03-llava-inference-engine/03-02-SUMMARY.md

<interfaces>
<!-- src/cinecut/inference/engine.py — built in plan 02 -->
```python
class LlavaEngine:
    base_url: str  # "http://127.0.0.1:{port}"
    def __enter__(self) -> "LlavaEngine": ...
    def __exit__(self, *_: object) -> None: ...
    # describe_frame() to be added in this plan
```

<!-- src/cinecut/inference/models.py — built in plan 02 -->
```python
@dataclass
class SceneDescription:
    visual_content: str
    mood: str
    action: str
    setting: str

SCENE_DESCRIPTION_SCHEMA: dict  # JSON schema for json_schema request field

def validate_scene_description(data: dict) -> SceneDescription: ...  # raises ValidationError on bad data
```

<!-- src/cinecut/models.py — Phase 1 -->
```python
@dataclass
class KeyframeRecord:
    timestamp_s: float
    frame_path: str     # Absolute path to JPEG
    source: str
```

<!-- llama-server /chat/completions request format (from RESEARCH.md, verified) -->
```python
payload = {
    "temperature": 0.1,
    "max_tokens": 256,
    "json_schema": SCENE_DESCRIPTION_SCHEMA,   # NOT response_format — use json_schema native field
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "Describe this film scene. Respond with a JSON object with keys: visual_content, mood, action, setting."},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}},
        ],
    }],
}
# Response: r.json()["choices"][0]["message"]["content"] is a string
# Parse with json.loads(content.strip()) then validate_scene_description()
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add describe_frame() and run_inference_stage() to engine.py</name>
  <files>
    src/cinecut/inference/engine.py
  </files>
  <action>
    Add two methods/functions to src/cinecut/inference/engine.py.

    **Method: `LlavaEngine.describe_frame(self, record: KeyframeRecord, timeout_s: float = 60.0) -> SceneDescription | None`**

    Per RESEARCH.md Pattern 2. Returns None on any error (never raises) so the caller can skip problem frames without aborting the pipeline.

    Implementation:
    ```python
    def describe_frame(
        self,
        record: KeyframeRecord,
        timeout_s: float = 60.0,
    ) -> "SceneDescription | None":
        """Submit one frame to LLaVA. Returns None on timeout/error (skip with warning)."""
        from cinecut.inference.models import SCENE_DESCRIPTION_SCHEMA, validate_scene_description
        from cinecut.models import KeyframeRecord as _KR  # already imported at module level; for type hint only
        import base64, json
        from pydantic import ValidationError

        img_bytes = Path(record.frame_path).read_bytes()
        b64 = base64.b64encode(img_bytes).decode("ascii")
        data_uri = f"data:image/jpeg;base64,{b64}"

        payload = {
            "temperature": 0.1,
            "max_tokens": 256,
            "json_schema": SCENE_DESCRIPTION_SCHEMA,
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "text", "text": (
                        "Describe this film scene. "
                        "Respond with a JSON object with keys: "
                        "visual_content, mood, action, setting."
                    )},
                    {"type": "image_url", "image_url": {"url": data_uri}},
                ],
            }],
        }
        try:
            r = requests.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                timeout=timeout_s,
            )
            r.raise_for_status()
            content = r.json()["choices"][0]["message"]["content"]
            return validate_scene_description(json.loads(content.strip()))
        except (requests.RequestException, KeyError, IndexError, json.JSONDecodeError, ValidationError) as exc:
            # Log warning — frame is skipped, pipeline continues
            # CLI caller is responsible for reporting skipped frames
            return None
    ```

    IMPORTANT — do NOT use `response_format` in the payload. Only `json_schema` (the native llama-server field). Using both causes HTTP 400 (RESEARCH.md Pitfall 4).

    **Module-level function: `run_inference_stage`**

    Add after the class definition:
    ```python
    def run_inference_stage(
        records: list,  # list[KeyframeRecord]
        model_path: Path,
        mmproj_path: Path,
        progress_callback: "Callable[[int, int], None] | None" = None,
    ) -> list:  # list[tuple[KeyframeRecord, SceneDescription | None]]
        """
        Run LLaVA inference on all keyframe records sequentially.
        Returns list of (record, scene_description_or_none) tuples.
        progress_callback(current: int, total: int) called after each frame.
        """
        results = []
        total = len(records)
        with LlavaEngine(model_path, mmproj_path) as engine:
            for i, record in enumerate(records):
                desc = engine.describe_frame(record)
                results.append((record, desc))
                if progress_callback:
                    progress_callback(i + 1, total)
        return results
    ```

    Add necessary imports at the top of engine.py if not already present:
    - `import base64`
    - `import json`
    - `import requests`
    - `from pathlib import Path`
    - `from typing import Callable`
    - `import subprocess`
    - `import time`
    - `from cinecut.errors import InferenceError, VramError`
    - `from cinecut.inference.vram import assert_vram_available`
    - `from cinecut.models import KeyframeRecord`

    The lazy imports inside describe_frame (SCENE_DESCRIPTION_SCHEMA, validate_scene_description) are fine to keep lazy or move to module level — either works. Keep them lazy if circular import is a concern, module-level if not.
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python3 -c "from cinecut.inference.engine import LlavaEngine, run_inference_stage; import inspect; assert hasattr(LlavaEngine, 'describe_frame'); assert callable(run_inference_stage); print('describe_frame + run_inference_stage OK')"</automated>
  </verify>
  <done>
    - LlavaEngine.describe_frame() exists and accepts (record: KeyframeRecord, timeout_s: float)
    - run_inference_stage() function exists and accepts (records, model_path, mmproj_path, progress_callback)
    - describe_frame uses json_schema (not response_format) in the payload
    - describe_frame returns None on any exception (does not raise)
  </done>
</task>

<task type="auto">
  <name>Task 2: Enable INFR-02 unit tests and wire inference into CLI</name>
  <files>
    tests/test_inference.py
    src/cinecut/cli.py
  </files>
  <action>
    **Part A: Enable INFR-02 unit tests in tests/test_inference.py**

    Remove the `pytest.skip` from `test_describe_frame_structure` and `test_malformed_response_skipped`. Replace with real test implementations using `unittest.mock.patch`.

    `test_describe_frame_structure`:
    - Create a fake JPEG file in tmp_path (1x1 JPEG bytes or just create a file with b"JPEG" content — describe_frame reads it via Path.read_bytes())
    - Create a KeyframeRecord with that path
    - Mock `requests.post` to return a MagicMock where `.json()` returns `{"choices": [{"message": {"content": '{"visual_content":"dark forest","mood":"tense","action":"man running","setting":"night woods"}'}}]}` and `.status_code = 200`
    - Also mock `.raise_for_status()` to do nothing
    - Import and instantiate LlavaEngine with dummy paths (do NOT start the server — only call describe_frame directly by bypassing the context manager). To do this, create the engine object without `__enter__`: `engine = LlavaEngine.__new__(LlavaEngine); engine.base_url = "http://127.0.0.1:8089"; engine._process = None; engine.debug = False`
    - Call `engine.describe_frame(record)`, assert result is SceneDescription with visual_content="dark forest", mood="tense", etc.

    `test_malformed_response_skipped`:
    - Same setup but mock `requests.post` to return content that is not valid JSON: `'not json at all'`
    - Assert `engine.describe_frame(record)` returns `None`

    Also update `test_no_model_reload` to no longer just skip — implement it as an integration test:
    - Keep the integration mark
    - Body: `with LlavaEngine(Path(LLAVA_MODEL_PATH), Path(MMPROJ_PATH)) as engine: pid1 = engine._process.pid; ... pid2 = engine._process.pid; assert pid1 == pid2`
    - This verifies the server is NOT restarted between frames

    **Part B: Wire inference stage into CLI**

    Read src/cinecut/cli.py first, then add the inference stage.

    The CLI currently has `--manifest` and `--review` flags. Add inference stage logic:
    1. After keyframe extraction completes (existing pipeline), add a conditional block: if no `--manifest` flag is provided (i.e., user wants full pipeline, not conform-only), run the inference stage.
    2. Import `run_inference_stage` from `cinecut.inference.engine` and `LLAVA_MODEL_PATH`, `MMPROJ_PATH` defaults.
    3. Add two new CLI options (optional, with defaults pointing to standard paths):
       - `--model`: Path to LLaVA GGUF model (default: `/home/adamh/models/ggml-model-q4_k.gguf`)
       - `--mmproj`: Path to mmproj GGUF (default: `/home/adamh/models/mmproj-model-f16.gguf`)
    4. Wrap the inference stage in a Rich progress display using `rich.progress.Progress` and `rich.progress.track` or a manual `Progress` context. Follow the existing Rich progress pattern in the CLI.
    5. The progress_callback passed to `run_inference_stage` should advance the Rich progress bar.
    6. After inference completes, print a summary: `console.print(f"[green]Inference complete:[/] {len(results)} frames processed, {skipped} skipped")`
    7. Store results in a local variable for use by Phase 4 manifest generation (Phase 4 will add the rest).

    If the CLI structure makes it awkward to add full inference wiring without Phase 4 context (manifest generation depends on inference output), at minimum add the Rich progress display and a stub that calls run_inference_stage and prints the summary. The key observable: running `cinecut <video> --subtitle <srt> --vibe action` shows inference progress in the CLI.

    Do NOT break the existing `--manifest` flow (conform-only path). The inference stage only runs when the full pipeline is invoked (no `--manifest` flag).
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python3 -m pytest tests/test_inference.py::test_describe_frame_structure tests/test_inference.py::test_malformed_response_skipped -x -q</automated>
  </verify>
  <done>
    - test_describe_frame_structure passes using mocked requests.post
    - test_malformed_response_skipped passes (returns None on malformed JSON)
    - CLI imports run_inference_stage without error
    - `python3 -m pytest tests/ -q` — full suite passes with no regressions
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
  <name>Checkpoint: Verify inference engine end-to-end</name>
  <action>Human verification checkpoint — no automated implementation. See how-to-verify for verification steps.</action>
    - LlavaEngine.describe_frame() with json_schema constrained output
    - run_inference_stage() for sequential frame processing
    - INFR-02 unit tests passing with mocked server
    - CLI wired with inference stage and Rich progress
    - Integration tests enabled (require model files to run)
  </what-built>
  <how-to-verify>
    1. Run unit tests: `python3 -m pytest tests/test_inference.py -v`
       - test_describe_frame_structure: PASS
       - test_malformed_response_skipped: PASS
       - test_vram_check: PASS
       - test_gpu_lock: PASS
       - Integration tests (test_server_health, test_no_model_reload): PASS if model files present, SKIP if not

    2. Run full test suite: `python3 -m pytest tests/ -q`
       - All existing tests must still pass (no regressions)

    3. Verify VRAM check works: `python3 -c "from cinecut.inference.vram import check_vram_free_mib; print(f'Free VRAM: {check_vram_free_mib()} MiB')"`
       - Should print actual free VRAM from nvidia-smi

    4. Smoke test LlavaEngine startup (requires model files at /home/adamh/models/):
       ```python
       python3 - <<'EOF'
       from pathlib import Path
       from cinecut.inference import LlavaEngine
       model = Path("/home/adamh/models/ggml-model-q4_k.gguf")
       mmproj = Path("/home/adamh/models/mmproj-model-f16.gguf")
       if model.exists() and mmproj.exists():
           with LlavaEngine(model, mmproj) as engine:
               print(f"Server healthy at {engine.base_url}")
       else:
           print("Model files not downloaded yet — skipping integration smoke test")
       EOF
       ```
       Expected: "Server healthy at http://127.0.0.1:8089" (if model present), or skip message

    5. Describe one frame (requires model files and a JPEG keyframe from a prior pipeline run):
       ```python
       python3 - <<'EOF'
       from pathlib import Path
       from cinecut.inference import LlavaEngine
       from cinecut.models import KeyframeRecord
       import glob

       model = Path("/home/adamh/models/ggml-model-q4_k.gguf")
       mmproj = Path("/home/adamh/models/mmproj-model-f16.gguf")
       # Find any JPEG keyframe from a previous run
       jpegs = glob.glob("/tmp/cinecut_*/**/*.jpg", recursive=True) or glob.glob("/home/adamh/**/*.jpg", recursive=True)[:1]

       if model.exists() and mmproj.exists() and jpegs:
           record = KeyframeRecord(timestamp_s=0.0, frame_path=jpegs[0], source="test")
           with LlavaEngine(model, mmproj) as engine:
               desc = engine.describe_frame(record)
               print(f"SceneDescription: {desc}")
       else:
           print("Skipping: model files or JPEG not available")
       EOF
       ```
       Expected: SceneDescription with visual_content, mood, action, setting populated
  </how-to-verify>
  <resume-signal>Type "approved" when unit tests pass and VRAM check works. Describe results of the integration smoke test (whether it ran and what was returned). Note any skipped items and why.</resume-signal>
</task>

</tasks>

<verification>
- `python3 -m pytest tests/test_inference.py -v` — test_describe_frame_structure, test_malformed_response_skipped, test_vram_check, test_gpu_lock all PASS
- `python3 -m pytest tests/ -q` — full suite green
- `python3 -c "from cinecut.inference.vram import check_vram_free_mib; print(check_vram_free_mib())"` prints integer (free VRAM in MiB)
- `from cinecut.inference.engine import run_inference_stage` imports without error
- Integration tests pass when model files present (SKIP when not present — acceptable)
</verification>

<success_criteria>
Phase 3 complete when:
- All 4 requirements covered: INFR-01 (LlavaEngine server lifecycle), INFR-02 (describe_frame structured output), INFR-03 (VRAM check + 6 GB guard + sequential processing), PIPE-05 (GPU_LOCK held during inference stage)
- Unit tests for INFR-02 pass with mocked server
- Integration test for INFR-01 passes with real model (or skips cleanly)
- Full test suite (65+ tests) green with no regressions
- Human verified VRAM check returns valid reading, inference engine importable, CLI shows inference progress
</success_criteria>

<output>
After completion, create `.planning/phases/03-llava-inference-engine/03-03-SUMMARY.md`
</output>
