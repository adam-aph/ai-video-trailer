---
phase: 03-llava-inference-engine
plan: 02
type: execute
wave: 2
depends_on:
  - 03-01
files_modified:
  - src/cinecut/inference/__init__.py
  - src/cinecut/inference/models.py
  - src/cinecut/inference/vram.py
  - src/cinecut/inference/engine.py
autonomous: true
requirements:
  - INFR-01
  - INFR-03
  - PIPE-05

must_haves:
  truths:
    - "GPU_LOCK is a module-level threading.Lock in cinecut.inference that serializes all GPU access"
    - "LlavaEngine is a context manager that starts llama-server on enter and terminates it on exit"
    - "LlavaEngine holds GPU_LOCK for its entire lifetime, preventing concurrent FFmpeg GPU operations"
    - "VRAM is checked before server startup and raises VramError if free memory is below 6 GB"
    - "Server stderr is discarded (not piped) to prevent pipe buffer deadlock"
    - "SceneDescription dataclass has visual_content, mood, action, setting fields"
  artifacts:
    - path: "src/cinecut/inference/__init__.py"
      provides: "GPU_LOCK, LlavaEngine, SceneDescription, InferenceError, VramError exports"
      exports: ["GPU_LOCK", "LlavaEngine", "SceneDescription"]
    - path: "src/cinecut/inference/models.py"
      provides: "SceneDescription dataclass + Pydantic TypeAdapter"
      contains: "SceneDescription"
    - path: "src/cinecut/inference/vram.py"
      provides: "check_vram_free_mib() + VRAM_MINIMUM_MIB constant"
      exports: ["check_vram_free_mib", "VRAM_MINIMUM_MIB"]
    - path: "src/cinecut/inference/engine.py"
      provides: "LlavaEngine context manager"
      exports: ["LlavaEngine"]
  key_links:
    - from: "src/cinecut/inference/engine.py"
      to: "src/cinecut/inference/__init__.py"
      via: "GPU_LOCK imported and acquired in __enter__"
      pattern: "GPU_LOCK\\.acquire"
    - from: "src/cinecut/inference/engine.py"
      to: "llama-server binary"
      via: "subprocess.Popen with DEVNULL stderr"
      pattern: "subprocess\\.Popen.*DEVNULL"
    - from: "src/cinecut/inference/vram.py"
      to: "nvidia-smi"
      via: "subprocess.run"
      pattern: "nvidia-smi.*memory\\.free"
---

<objective>
Build the inference module: SceneDescription dataclass (models.py), VRAM pre-flight check (vram.py), and the LlavaEngine context manager (engine.py) with full server lifecycle management, GPU_LOCK serialization, and health polling.

Purpose: This is the core engine for INFR-01, INFR-03, and PIPE-05. The context manager pattern guarantees no zombie llama-server processes. GPU_LOCK guarantees sequential GPU access for Phase 5 when inference and FFmpeg conform run in the same pipeline.
Output: A fully importable cinecut.inference package with LlavaEngine, SceneDescription, GPU_LOCK, check_vram_free_mib.
</objective>

<execution_context>
@/home/adamh/.claude/get-shit-done/workflows/execute-plan.md
@/home/adamh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-llava-inference-engine/03-RESEARCH.md
@.planning/phases/03-llava-inference-engine/03-01-SUMMARY.md

<interfaces>
<!-- src/cinecut/errors.py — added in plan 01 -->
```python
class InferenceError(CineCutError):
    def __init__(self, detail: str) -> None: ...
    self.detail: str

class VramError(CineCutError):
    def __init__(self, detail: str) -> None: ...
    self.detail: str
```

<!-- src/cinecut/models.py — Phase 1 shared models -->
```python
@dataclass
class KeyframeRecord:
    timestamp_s: float
    frame_path: str     # Absolute path to JPEG
    source: str
```

<!-- llama-server launch command (from RESEARCH.md, verified flags) -->
```
llama-server
  -m <model_path>
  --mmproj <mmproj_path>
  --port <port>
  --host 127.0.0.1
  -ngl 99           # all layers to GPU
  -c 2048           # context size
  -np 1             # 1 slot, sequential
  --log-disable     # suppress console noise
  stdout=DEVNULL, stderr=DEVNULL  # critical: PIPE causes buffer deadlock
```

<!-- Health endpoint -->
```
GET /health
  HTTP 200: {"status": "ok"}    # ready
  HTTP 503: loading             # still loading
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create inference package with SceneDescription model and VRAM check</name>
  <files>
    src/cinecut/inference/__init__.py
    src/cinecut/inference/models.py
    src/cinecut/inference/vram.py
  </files>
  <action>
    Create the `src/cinecut/inference/` directory and three files.

    **src/cinecut/inference/models.py:**
    SceneDescription dataclass + SCENE_DESCRIPTION_SCHEMA dict + Pydantic TypeAdapter for validation.
    - Use stdlib `dataclasses.dataclass` (consistent with Phase 1 pattern — DialogueEvent, KeyframeRecord)
    - All four fields are `str` type: visual_content, mood, action, setting
    - Add a Pydantic TypeAdapter for runtime validation: `from pydantic import TypeAdapter; _adapter = TypeAdapter(SceneDescription)`
    - Export `validate_scene_description(data: dict) -> SceneDescription` that calls `_adapter.validate_python(data)` (raises pydantic.ValidationError on bad data)
    - Export `SCENE_DESCRIPTION_SCHEMA` dict (the JSON schema for llama-server json_schema field):
      ```python
      SCENE_DESCRIPTION_SCHEMA = {
          "type": "object",
          "properties": {
              "visual_content": {"type": "string"},
              "mood": {"type": "string"},
              "action": {"type": "string"},
              "setting": {"type": "string"},
          },
          "required": ["visual_content", "mood", "action", "setting"],
          "additionalProperties": False,
      }
      ```

    **src/cinecut/inference/vram.py:**
    VRAM pre-flight check via nvidia-smi.
    - `VRAM_MINIMUM_MIB: int = 6144` (6 GB minimum for LLaVA 1.5-7B Q4 + context)
    - `check_vram_free_mib() -> int`: runs `nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits`, returns int MiB. Raises `VramError` (imported from cinecut.errors) if subprocess fails. Uses `subprocess.run(..., capture_output=True, text=True, check=True, timeout=10)`. Parses `result.stdout.strip().splitlines()[0]` as int. Wraps CalledProcessError, ValueError, IndexError in VramError.
    - `assert_vram_available() -> None`: calls check_vram_free_mib(), raises VramError with message "Only {free} MiB free VRAM, need at least {VRAM_MINIMUM_MIB} MiB" if free < VRAM_MINIMUM_MIB.

    **src/cinecut/inference/__init__.py:**
    Module-level GPU_LOCK and re-exports.
    ```python
    """CineCut inference package — LLaVA engine, VRAM management, GPU serialization."""
    import threading

    # Module-level lock serializing all GPU operations across the process.
    # LlavaEngine acquires this on __enter__ and releases on __exit__.
    # FFmpeg conform pipeline must acquire this lock before any GPU operations (Phase 5).
    GPU_LOCK: threading.Lock = threading.Lock()

    from cinecut.inference.engine import LlavaEngine  # noqa: E402
    from cinecut.inference.models import SceneDescription, SCENE_DESCRIPTION_SCHEMA  # noqa: E402
    from cinecut.inference.vram import check_vram_free_mib, VRAM_MINIMUM_MIB  # noqa: E402

    __all__ = [
        "GPU_LOCK",
        "LlavaEngine",
        "SceneDescription",
        "SCENE_DESCRIPTION_SCHEMA",
        "check_vram_free_mib",
        "VRAM_MINIMUM_MIB",
    ]
    ```
    NOTE: engine.py (imported in __init__.py) must be created before this import chain works. Create engine.py in Task 2 — if needed, stub __init__.py without the engine import first and add it after Task 2 completes. Alternatively, create all three files in the correct order: models.py, vram.py, engine.py, then __init__.py.
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python3 -c "from cinecut.inference.models import SceneDescription, SCENE_DESCRIPTION_SCHEMA, validate_scene_description; from cinecut.inference.vram import check_vram_free_mib, VRAM_MINIMUM_MIB, assert_vram_available; print('models+vram OK')"</automated>
  </verify>
  <done>
    - cinecut.inference.models importable with SceneDescription, SCENE_DESCRIPTION_SCHEMA, validate_scene_description
    - cinecut.inference.vram importable with check_vram_free_mib, VRAM_MINIMUM_MIB, assert_vram_available
    - SceneDescription is a dataclass with visual_content, mood, action, setting (all str)
    - VRAM_MINIMUM_MIB = 6144
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement LlavaEngine context manager and complete the package</name>
  <files>
    src/cinecut/inference/engine.py
    src/cinecut/inference/__init__.py
  </files>
  <action>
    Create src/cinecut/inference/engine.py implementing the LlavaEngine context manager per RESEARCH.md Pattern 1.

    **Full implementation requirements:**

    Class: `LlavaEngine`
    Constructor args: `model_path: Path, mmproj_path: Path, port: int = 8089, debug: bool = False`
    - `self.base_url = f"http://127.0.0.1:{port}"`
    - `self._process: subprocess.Popen | None = None`
    - `self._log_file = None` (used in debug mode)
    - `self.debug = debug`

    `__enter__(self) -> "LlavaEngine"`:
    1. Import GPU_LOCK from cinecut.inference — IMPORTANT: to avoid circular import, import it lazily inside the method: `from cinecut.inference import GPU_LOCK`
    2. Call `assert_vram_available()` (from cinecut.inference.vram) — raises VramError before acquiring lock if VRAM insufficient
    3. `GPU_LOCK.acquire()` — hold lock for entire inference stage
    4. Call `self._start()`
    5. Return `self`

    `__exit__(self, *_: object) -> None`:
    1. Call `self._stop()`
    2. `from cinecut.inference import GPU_LOCK; GPU_LOCK.release()` — always release even if stop raises

    `_start(self) -> None`:
    - Build cmd list (use RESEARCH.md llama-server launch command exactly):
      ```python
      cmd = [
          "llama-server",
          "-m", str(self.model_path),
          "--mmproj", str(self.mmproj_path),
          "--port", str(self.port),
          "--host", "127.0.0.1",
          "-ngl", "99",
          "-c", "2048",
          "-np", "1",
          "--log-disable",
      ]
      ```
    - If `self.debug` is True: open a log file at `Path(self.model_path).parent / "llama-server.log"`, set `self._log_file` to the file object, use it for both stdout and stderr in Popen.
    - If `self.debug` is False: `stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL` (CRITICAL: never use PIPE — pipe buffer fills and deadlocks the server per RESEARCH.md Pitfall 1)
    - Launch: `self._process = subprocess.Popen(cmd, stdout=..., stderr=...)`
    - Call `self._wait_for_health(timeout_s=120)`

    `_wait_for_health(self, timeout_s: float) -> None`:
    - Import `time`, `requests` at top of file
    - `deadline = time.monotonic() + timeout_s`
    - Loop while `time.monotonic() < deadline`:
      - If `self._process.poll() is not None`: raise `InferenceError("llama-server exited during startup — check model path and VRAM")` (per RESEARCH.md Pitfall 2: detect early exit)
      - Try `requests.get(f"{self.base_url}/health", timeout=2)`, check status 200 + `r.json().get("status") == "ok"` → return
      - Except `requests.RequestException`: pass
      - `time.sleep(1.0)`
    - After loop: `self._process.terminate(); self._process.wait(timeout=10)` then raise `InferenceError(f"llama-server did not become healthy within {timeout_s}s")`

    `_stop(self) -> None`:
    - If `self._process` is not None and `self._process.poll() is None`:
      - `self._process.terminate()`
      - Try `self._process.wait(timeout=10)`, except subprocess.TimeoutExpired: `self._process.kill(); self._process.wait()`
    - Set `self._process = None`
    - If `self._log_file` is not None: close it, set to None

    After creating engine.py, update src/cinecut/inference/__init__.py to include the engine import so the full public API is available. The __init__.py should be as written in Task 1 but with the `from cinecut.inference.engine import LlavaEngine` line active (not commented out).

    To avoid circular import (engine.py imports GPU_LOCK from __init__.py at runtime via lazy import in __enter__/__exit__): use the lazy import pattern `from cinecut.inference import GPU_LOCK` inside the method body, not at module top level.
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python3 -c "from cinecut.inference import GPU_LOCK, LlavaEngine, SceneDescription; import threading; assert isinstance(GPU_LOCK, type(threading.Lock())); print('engine OK')"</automated>
  </verify>
  <done>
    - cinecut.inference importable with GPU_LOCK, LlavaEngine, SceneDescription
    - GPU_LOCK is a threading.Lock instance
    - LlavaEngine has __enter__, __exit__, _start, _stop, _wait_for_health methods
    - No circular import errors
    - Server stderr uses DEVNULL (not PIPE) in non-debug mode
    - GPU_LOCK is acquired in __enter__ and released in __exit__
    - `python3 -m pytest tests/test_inference.py::test_gpu_lock -x -q` passes
  </done>
</task>

</tasks>

<verification>
- `python3 -c "from cinecut.inference import GPU_LOCK, LlavaEngine, SceneDescription, check_vram_free_mib; print('all imports OK')"` prints without error
- `python3 -m pytest tests/test_inference.py -x -q` — test_gpu_lock and test_vram_check now pass (no longer skipped by importorskip)
- `python3 -m pytest tests/ -q` — full existing test suite still green (no regressions)
</verification>

<success_criteria>
- cinecut.inference package exists with __init__.py, models.py, vram.py, engine.py
- GPU_LOCK is threading.Lock, acquired on LlavaEngine __enter__, released on __exit__
- LlavaEngine launches llama-server with DEVNULL stderr (no pipe deadlock risk)
- VramError raised when VRAM below 6 GB threshold
- InferenceError raised if server exits during startup or health poll times out
- Full test suite passes with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/03-llava-inference-engine/03-02-SUMMARY.md`
</output>
