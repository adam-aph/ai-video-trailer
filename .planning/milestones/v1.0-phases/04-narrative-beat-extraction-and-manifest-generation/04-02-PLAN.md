---
phase: 04-narrative-beat-extraction-and-manifest-generation
plan: "02"
type: execute
wave: 2
depends_on:
  - "04-01"
files_modified:
  - src/cinecut/narrative/generator.py
  - src/cinecut/cli.py
  - tests/test_narrative.py
autonomous: true
requirements:
  - EDIT-01

must_haves:
  truths:
    - "run_narrative_stage() accepts inference results + dialogue events + vibe + source_file + work_dir and returns Path to written TRAILER_MANIFEST.json"
    - "Generated manifest contains exactly clip_count_max clips (or all scenes if fewer available), sorted chronologically, each with beat_type, act, source timecodes, reasoning, visual_analysis, subtitle_analysis, and money_shot_score"
    - "Written TRAILER_MANIFEST.json loads cleanly via load_manifest() and validates against TrailerManifest schema"
    - "CLI Stage 5 runs run_narrative_stage() after Stage 4 inference completes, writes manifest, and prints progress with Rich"
    - "No clip overlap: each clip's source_start_s >= previous clip's source_end_s (0.5s minimum gap enforced)"
    - "Unit tests for NARR-02, NARR-03, and EDIT-01 pass (beat classification, scoring, manifest generation)"
  artifacts:
    - path: "src/cinecut/narrative/generator.py"
      provides: "Manifest assembly from scored/classified scenes; JSON write"
      exports: ["run_narrative_stage"]
    - path: "src/cinecut/cli.py"
      provides: "Stage 5 narrative stage wired into full pipeline"
      contains: "run_narrative_stage"
    - path: "tests/test_narrative.py"
      provides: "Unit tests for NARR-02, NARR-03, EDIT-01"
      min_lines: 80
  key_links:
    - from: "src/cinecut/narrative/generator.py"
      to: "src/cinecut/narrative/signals.py"
      via: "extract_all_signals, get_film_duration_s"
      pattern: "from cinecut.narrative.signals import"
    - from: "src/cinecut/narrative/generator.py"
      to: "src/cinecut/narrative/scorer.py"
      via: "normalize_all_signals, compute_money_shot_score, classify_beat, assign_act"
      pattern: "from cinecut.narrative.scorer import"
    - from: "src/cinecut/narrative/generator.py"
      to: "src/cinecut/manifest/schema.py"
      via: "ClipEntry (with extended fields), TrailerManifest"
      pattern: "from cinecut.manifest.schema import"
    - from: "src/cinecut/narrative/generator.py"
      to: "src/cinecut/manifest/vibes.py"
      via: "VIBE_PROFILES[vibe] for clip duration + transition"
      pattern: "from cinecut.manifest.vibes import"
    - from: "src/cinecut/cli.py"
      to: "src/cinecut/narrative/generator.py"
      via: "run_narrative_stage(inference_results, dialogue_events, vibe, video, work_dir)"
      pattern: "from cinecut.narrative.generator import"
---

<objective>
Implement generator.py (manifest assembly pipeline), add CLI Stage 5 wiring, and write unit tests covering all three Phase 4 requirements.

Purpose: This plan delivers the complete pipeline from raw inference results to a persisted TRAILER_MANIFEST.json file, and connects it to the CLI so `cinecut <film> --subtitle <srt> --vibe <name>` runs the full pipeline through manifest generation.

Output: generator.py, updated cli.py, tests/test_narrative.py.
</objective>

<execution_context>
@/home/adamh/.claude/get-shit-done/workflows/execute-plan.md
@/home/adamh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-narrative-beat-extraction-and-manifest-generation/04-01-SUMMARY.md

<interfaces>
<!-- Key types and contracts. No codebase exploration needed. -->

From src/cinecut/narrative/signals.py (created in Plan 01):
```python
@dataclass
class RawSignals:
    motion_magnitude: float
    visual_contrast: float
    scene_uniqueness: float
    subtitle_emotional_weight: float
    face_presence: float
    llava_confidence: float
    saturation: float
    chronological_position: float

def extract_all_signals(
    records: list[KeyframeRecord],
    scene_descriptions: list[SceneDescription | None],
    dialogue_events: list[DialogueEvent],
    film_duration_s: float,
) -> list[RawSignals]: ...

def get_film_duration_s(source_file: Path) -> float: ...
```

From src/cinecut/narrative/scorer.py (created in Plan 01):
```python
SIGNAL_WEIGHTS: dict[str, float]  # 8 keys, sum = 1.0

def normalize_all_signals(raw_signals: list[RawSignals]) -> list[dict[str, float]]: ...
def compute_money_shot_score(normalized: dict[str, float]) -> float: ...
def classify_beat(chron_pos: float, emotion: str, money_shot_score: float, has_face: bool) -> str: ...
def assign_act(chron_pos: float, beat_type: str) -> str: ...
```

From src/cinecut/manifest/schema.py (extended in Plan 01):
```python
class ClipEntry(BaseModel):
    source_start_s: float
    source_end_s: float
    beat_type: Literal["inciting_incident", "character_introduction", "escalation_beat",
                       "relationship_beat", "money_shot", "climax_peak", "breath"]
    act: Literal["cold_open", "act1", "beat_drop", "act2", "breath", "act3", "title_card", "button"]
    transition: Literal["hard_cut", "crossfade", "fade_to_black", "fade_to_white"] = "hard_cut"
    dialogue_excerpt: str = ""
    reasoning: Optional[str] = None
    visual_analysis: Optional[str] = None
    subtitle_analysis: Optional[str] = None
    money_shot_score: Optional[float] = None  # ge=0.0, le=1.0

class TrailerManifest(BaseModel):
    schema_version: str = "1.0"
    source_file: str
    vibe: str
    clips: list[ClipEntry]
```

From src/cinecut/manifest/vibes.py:
```python
@dataclass(frozen=True)
class VibeProfile:
    name: str
    act1_avg_cut_s: float
    act2_avg_cut_s: float
    act3_avg_cut_s: float
    clip_count_min: int
    clip_count_max: int
    dialogue_ratio: float
    primary_transition: str    # "hard_cut" | "crossfade" | "fade_to_black"
    secondary_transition: str  # used at act boundaries
    # ... other fields

VIBE_PROFILES: dict[str, VibeProfile]
```

From src/cinecut/manifest/loader.py:
```python
def load_manifest(path: Path) -> TrailerManifest: ...
```

From src/cinecut/inference/engine.py:
```python
def run_inference_stage(records, model_path, mmproj_path, progress_callback=None) -> list:
    # returns list[tuple[KeyframeRecord, SceneDescription | None]]
```

From src/cinecut/cli.py (Stage 4 currently ends with inference_results):
```python
# After Stage 4 inference:
inference_results = run_inference_stage(...)  # list[tuple[KeyframeRecord, SceneDescription | None]]
# Stage 5 narrative generation goes here before the existing conform block
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement generator.py — manifest assembly pipeline</name>
  <files>src/cinecut/narrative/generator.py</files>
  <action>
Create `src/cinecut/narrative/generator.py`. This module assembles a TrailerManifest from scored and classified scenes.

**Imports:**
```python
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, Optional

from cinecut.manifest.schema import ClipEntry, TrailerManifest
from cinecut.manifest.vibes import VIBE_PROFILES
from cinecut.models import DialogueEvent, KeyframeRecord
from cinecut.narrative.signals import RawSignals, extract_all_signals, get_film_duration_s
from cinecut.narrative.scorer import (
    assign_act, classify_beat, compute_money_shot_score, normalize_all_signals,
)
```

**compute_clip_window(timestamp_s, act, vibe_profile, film_duration_s) -> tuple[float, float]:**
Map act to duration: `cold_open → act1_avg_cut_s`, `act1 → act1_avg_cut_s`, `beat_drop → act2_avg_cut_s`, `act2 → act2_avg_cut_s`, `breath → act2_avg_cut_s * 1.5`, `act3 → act3_avg_cut_s`, else `act2_avg_cut_s`. Bias 30% before keyframe, 70% after: `start = max(0.0, timestamp_s - duration * 0.3)`, `end = min(film_duration_s, timestamp_s + duration * 0.7)`. Return `(start, end)`.

**resolve_overlaps(windows: list[tuple[float, float]]) -> list[tuple[float, float]]:**
Given sorted windows, check each adjacent pair. If `windows[i][1] > windows[i+1][0]`, shorten clip i's end to `windows[i+1][0] - 0.5` (0.5s gap). If after shortening the clip would have end <= start, set end = start + 0.1 (degenerate — will be filtered by ClipEntry validator, but keep for safety). Return fixed list.

**get_dialogue_excerpt(timestamp_s, dialogue_events, window_s=5.0) -> str:**
Return the text of the nearest DialogueEvent within window_s. If none found, return empty string. Prefer events where `event.start_s <= timestamp_s <= event.end_s` (direct overlap). Among proximity matches, pick the one with smallest distance to event midpoint.

**get_transition(act, vibe_profile) -> str:**
Act boundaries (cold_open, beat_drop, act3) use `vibe_profile.secondary_transition`. All others use `vibe_profile.primary_transition`.

**build_reasoning(record, desc, beat_type, score) -> str:**
Construct a short reasoning string: `f"Beat: {beat_type}. Score: {score:.2f}. Source: {record.source}. " + (f"Visual: {desc.mood}, {desc.action}." if desc else "No visual description.")`

**run_narrative_stage(inference_results, dialogue_events, vibe, source_file, work_dir, progress_callback=None) -> Path:**
Full pipeline:
1. `film_duration_s = get_film_duration_s(source_file)`
2. Unpack `inference_results` into parallel lists: `records = [r for r, _ in inference_results]`, `descriptions = [d for _, d in inference_results]`
3. `raw_signals = extract_all_signals(records, descriptions, dialogue_events, film_duration_s)`
4. `normalized = normalize_all_signals(raw_signals)`
5. For each i: compute `score = compute_money_shot_score(normalized[i])`, get `emotion` from nearest dialogue event (`get_subtitle_emotional_weight` analog — look up emotion string, not float: find nearest event with same window logic, return event.emotion or "neutral" if none), compute `has_face = raw_signals[i].face_presence > 0.5`, classify beat, assign act.

   For the emotion string lookup: write a small `get_nearest_emotion(timestamp_s, dialogue_events, window_s=5.0) -> str` function that returns the emotion string (not float) of the nearest event within window_s, or "neutral" if none found.

6. Build scored list: list of dicts with record, desc, score, beat_type, act, emotion for sorting.
7. Sort by score descending, take top N where `N = min(len(records), VIBE_PROFILES[vibe].clip_count_max)`.
8. Sort the selected N by `record.timestamp_s` ascending (chronological order for manifest).
9. Compute clip windows for selected records using `compute_clip_window`.
10. Call `resolve_overlaps` on the windows list.
11. For each selected record (in chronological order), build `ClipEntry`:
    - `source_start_s`, `source_end_s` from resolved windows
    - `beat_type`, `act` from classification
    - `transition = get_transition(act, VIBE_PROFILES[vibe])`
    - `dialogue_excerpt = get_dialogue_excerpt(record.timestamp_s, dialogue_events)`
    - `reasoning = build_reasoning(record, desc, beat_type, score)`
    - `visual_analysis = f"{desc.visual_content}. {desc.mood}. {desc.action}. {desc.setting}." if desc else None`
    - `subtitle_analysis = f"Emotion: {emotion}. Weight: {raw_signals[i].subtitle_emotional_weight:.2f}." if emotion != "neutral" else None`
    - `money_shot_score = round(score, 4)`
    - Skip clips where end <= start (degenerate window from resolve_overlaps).
12. Assemble `TrailerManifest(source_file=str(source_file), vibe=vibe, clips=clip_entries)`.
13. Write manifest: `output_path = work_dir / "TRAILER_MANIFEST.json"`, write `manifest.model_dump_json(indent=2, exclude_none=True)` as UTF-8. Return output_path.
14. Call `progress_callback(i, total)` after each clip is processed if callback is not None.

**Signature:**
```python
def run_narrative_stage(
    inference_results: list,       # list[tuple[KeyframeRecord, SceneDescription | None]]
    dialogue_events: list,         # list[DialogueEvent]
    vibe: str,                     # e.g. "action"
    source_file: Path,             # original MKV/AVI/MP4 (NOT the proxy)
    work_dir: Path,                # directory to write TRAILER_MANIFEST.json
    progress_callback: Callable[[int, int], None] | None = None,
) -> Path:                         # path to written TRAILER_MANIFEST.json
```
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python3 -c "from cinecut.narrative.generator import run_narrative_stage; print('import OK')" 2>&1</automated>
  </verify>
  <done>run_narrative_stage importable; module has no syntax errors; all internal functions defined (compute_clip_window, resolve_overlaps, get_dialogue_excerpt, get_transition, build_reasoning, get_nearest_emotion)</done>
</task>

<task type="auto">
  <name>Task 2: Wire CLI Stage 5 and write test_narrative.py</name>
  <files>
    src/cinecut/cli.py
    tests/test_narrative.py
  </files>
  <action>
**Part A: CLI Stage 5 wiring (src/cinecut/cli.py)**

After Stage 4 inference completes (after the `skipped` count print), add Stage 5 before the existing "Summary" Panel. Replace the existing Phase 3 Complete panel with a combined ingestion+inference summary, then run Stage 5:

1. Add import at top of cli.py: `from cinecut.narrative.generator import run_narrative_stage`
2. Replace the "Phase 3 Complete" summary panel text with "Ingestion + Inference complete" (already says that).
3. After the inference summary console.print, add Stage 5:

```python
# --- Stage 5: Narrative beat extraction and manifest generation (NARR-02, NARR-03, EDIT-01) ---
console.print("[bold]Stage 5/5:[/bold] Extracting narrative beats and generating manifest...")

with Progress(
    SpinnerColumn(),
    TextColumn("[progress.description]{task.description}"),
    BarColumn(),
    TextColumn("{task.completed}/{task.total}"),
    TimeElapsedColumn(),
    console=console,
) as progress:
    narr_task = progress.add_task(
        "Scoring and classifying scenes...", total=len(inference_results)
    )

    def _narr_callback(current: int, total: int) -> None:
        progress.update(narr_task, completed=current)

    manifest_path = run_narrative_stage(
        inference_results,
        dialogue_events,
        vibe_normalized,
        video,      # original source, NOT proxy
        work_dir,
        progress_callback=_narr_callback,
    )

console.print(f"[green]Manifest written: [dim]{manifest_path.name}[/dim]\n")
```

4. Update the summary Panel to include manifest info:
```python
console.print(Panel(
    f"[bold green]Pipeline complete[/bold green]\n\n"
    f"  Proxy:      [dim]{proxy_path.name}[/dim]\n"
    f"  Subtitles:  {len(dialogue_events)} events\n"
    f"  Keyframes:  {len(keyframe_records)} frames\n"
    f"  Described:  {len(inference_results) - skipped} frames ({skipped} skipped)\n"
    f"  Manifest:   [dim]{manifest_path.name}[/dim]\n"
    f"  Work dir:   [dim]{work_dir}[/dim]",
    title="[green]Phase 4 Complete[/green]",
    border_style="green",
))
```

5. Set `trailer_manifest = load_manifest(manifest_path)` after Stage 5 so the existing --review / conform logic can use the generated manifest. Change the existing `trailer_manifest = None` initialization to remain `None`, and only set it here after generation. The conform block is already guarded by `if trailer_manifest is not None`.

**Part B: Unit tests (tests/test_narrative.py)**

Write unit tests that cover NARR-02, NARR-03, and EDIT-01 without needing real JPEG files or ffprobe. Use unittest.mock and tmp_path.

```python
"""Unit tests for Phase 4 narrative analysis: NARR-02, NARR-03, EDIT-01."""
import json
from pathlib import Path
from unittest import mock

import pytest
```

Test groups:

**NARR-02: Beat classification (scorer.classify_beat)**
- `test_beat_classify_breath`: score=0.1, emotion="neutral" → "breath"
- `test_beat_classify_climax_peak`: chron_pos=0.85, score=0.75 → "climax_peak"
- `test_beat_classify_money_shot`: score=0.90 → "money_shot"
- `test_beat_classify_character_intro`: chron_pos=0.05, has_face=True, emotion="positive" → "character_introduction"
- `test_beat_classify_inciting_incident`: chron_pos=0.20, emotion="intense", score=0.40 → "inciting_incident"
- `test_beat_classify_relationship`: emotion="romantic", has_face=True, score=0.50 → "relationship_beat"
- `test_beat_classify_escalation_fallback`: emotion="negative", chron_pos=0.50, score=0.40, has_face=False → "escalation_beat"
- `test_act_breath_overrides_position`: chron_pos=0.9, beat_type="breath" → act="breath"
- `test_act_cold_open`: chron_pos=0.03, beat_type="escalation_beat" → act="cold_open"
- `test_act_act3`: chron_pos=0.85, beat_type="money_shot" → act="act3"

**NARR-03: Signal scoring (scorer.compute_money_shot_score + normalize_signal_pool)**
- `test_signal_weights_sum_to_one`: assert abs(sum(SIGNAL_WEIGHTS.values()) - 1.0) < 1e-9
- `test_money_shot_score_range`: build a normalized dict with all 0.5, score should be 0.5
- `test_money_shot_score_max`: all 1.0 → score is 1.0
- `test_money_shot_score_min`: all 0.0 → score is 0.0
- `test_normalize_uniform`: [5.0, 5.0, 5.0] → [0.5, 0.5, 0.5]
- `test_normalize_range`: [0.0, 10.0, 5.0] → [0.0, 1.0, 0.5]

**EDIT-01: Manifest generation (generator.run_narrative_stage)**
Write a test using mocks that patches `get_film_duration_s` (returns 120.0) and uses synthetic KeyframeRecord + SceneDescription objects with real JPEG-less computation (mock `cv2.imread` to return None so image signals all zero, which is acceptable for the test).

```python
def test_run_narrative_stage_writes_manifest(tmp_path):
    """EDIT-01: run_narrative_stage writes a valid TRAILER_MANIFEST.json."""
    from cinecut.models import KeyframeRecord, DialogueEvent
    from cinecut.inference.models import SceneDescription
    from cinecut.narrative.generator import run_narrative_stage
    from cinecut.manifest.loader import load_manifest

    records = [
        KeyframeRecord(timestamp_s=float(t), frame_path=str(tmp_path / f"f{t:04d}.jpg"), source="subtitle_midpoint")
        for t in range(0, 120, 5)  # 24 frames
    ]
    descriptions = [
        SceneDescription(visual_content="dark forest", mood="tense", action="man running", setting="night")
        for _ in records
    ]
    dialogue_events = [
        DialogueEvent(start_ms=0, end_ms=4000, start_s=0.0, end_s=4.0, midpoint_s=2.0,
                      text="We must act now.", emotion="intense")
    ]
    inference_results = list(zip(records, descriptions))

    with mock.patch("cinecut.narrative.signals.get_film_duration_s", return_value=120.0), \
         mock.patch("cv2.imread", return_value=None):  # no real JPEGs needed
        manifest_path = run_narrative_stage(
            inference_results, dialogue_events, "action",
            source_file=tmp_path / "film.mkv",
            work_dir=tmp_path,
        )

    assert manifest_path.exists()
    assert manifest_path.name == "TRAILER_MANIFEST.json"
    loaded = load_manifest(manifest_path)
    assert loaded.vibe == "action"
    assert len(loaded.clips) >= 1
    for clip in loaded.clips:
        assert clip.beat_type in {
            "inciting_incident", "character_introduction", "escalation_beat",
            "relationship_beat", "money_shot", "climax_peak", "breath",
        }
        assert clip.source_end_s > clip.source_start_s
        assert clip.money_shot_score is not None
```

Also add `test_no_clip_overlap`:
```python
def test_no_clip_overlap(tmp_path):
    """EDIT-01: Adjacent clips in generated manifest do not overlap."""
    # same setup as above, then:
    # assert all(loaded.clips[i].source_end_s <= loaded.clips[i+1].source_start_s
    #            for i in range(len(loaded.clips) - 1))
```
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python3 -m pytest tests/test_narrative.py -x -q 2>&1 | tail -15</automated>
  </verify>
  <done>All test_narrative.py tests pass; CLI imports run_narrative_stage without error; python3 -m pytest tests/ -x -q --ignore=tests/test_inference.py passes (no regressions)</done>
</task>

</tasks>

<verification>
Full regression suite:
```bash
cd /home/adamh/ai-video-trailer && python3 -m pytest tests/ -x -q --ignore=tests/test_inference.py 2>&1 | tail -15
```

Verify manifest schema is backward compatible (existing fixture still loads):
```bash
cd /home/adamh/ai-video-trailer && python3 -m pytest tests/test_manifest.py -v 2>&1 | tail -10
```

Verify CLI imports cleanly with Stage 5 wired:
```bash
cd /home/adamh/ai-video-trailer && python3 -c "from cinecut.cli import app; print('CLI import OK')"
```
</verification>

<success_criteria>
- run_narrative_stage() is importable and wired in CLI
- Generated manifest validates against TrailerManifest schema
- Adjacent clips have no overlap (source_end_s <= next source_start_s)
- All clips have beat_type from the 7 valid literals
- All clips have money_shot_score in [0.0, 1.0]
- tests/test_narrative.py: all tests pass
- Full test suite passes with no regressions (71+ tests pass)
</success_criteria>

<output>
After completion, create `.planning/phases/04-narrative-beat-extraction-and-manifest-generation/04-02-SUMMARY.md`
</output>
