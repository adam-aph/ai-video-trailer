---
phase: 03-llava-inference-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/cinecut/errors.py
  - tests/test_inference.py
autonomous: false
requirements:
  - INFR-01
  - INFR-02
  - INFR-03
  - PIPE-05
user_setup:
  - service: llava-model
    why: "LLaVA 1.5-7B GGUF and mmproj files must exist locally before inference engine can run"
    env_vars: []
    dashboard_config:
      - task: "Download LLaVA 1.5-7B model files"
        location: |
          Run these two wget commands (approx 4.5 GB total):
            mkdir -p /home/adamh/models
            wget -c "https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/ggml-model-q4_k.gguf" \
                 -O /home/adamh/models/ggml-model-q4_k.gguf
            wget -c "https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/mmproj-model-f16.gguf" \
                 -O /home/adamh/models/mmproj-model-f16.gguf
          Verify: ls -lh /home/adamh/models/*.gguf

must_haves:
  truths:
    - "LLaVA model and mmproj files exist at /home/adamh/models/ on disk"
    - "requests library is declared in pyproject.toml dependencies"
    - "InferenceError and VramError exist in errors.py and follow project error conventions"
    - "Test scaffold exists at tests/test_inference.py covering all Phase 3 requirements"
  artifacts:
    - path: "pyproject.toml"
      provides: "requests declared as dependency"
      contains: "requests"
    - path: "src/cinecut/errors.py"
      provides: "InferenceError, VramError error classes"
      exports: ["InferenceError", "VramError"]
    - path: "tests/test_inference.py"
      provides: "Test scaffold for INFR-01, INFR-02, INFR-03, PIPE-05"
      min_lines: 50
  key_links:
    - from: "tests/test_inference.py"
      to: "src/cinecut/errors.py"
      via: "import InferenceError, VramError"
      pattern: "from cinecut\\.errors import"
---

<objective>
Set up the Phase 3 infrastructure: declare the requests dependency, add InferenceError and VramError error classes to the project's shared errors module, create the test scaffold, and guide the user through the one-time LLaVA model download.

Purpose: Without the model files on disk, no inference can run. Without error classes, the inference module cannot be written. Without the test scaffold, subsequent plans have nowhere to put tests.
Output: pyproject.toml with requests declared, errors.py with two new error classes, tests/test_inference.py scaffold, and LLaVA model files at /home/adamh/models/.
</objective>

<execution_context>
@/home/adamh/.claude/get-shit-done/workflows/execute-plan.md
@/home/adamh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llava-inference-engine/03-RESEARCH.md

<interfaces>
<!-- Existing error class pattern from src/cinecut/errors.py — new errors must match this style -->
```python
class CineCutError(Exception):
    """Base class for all CineCut errors."""

class ProxyCreationError(CineCutError):
    def __init__(self, source: Path, detail: str) -> None:
        super().__init__(
            f"Failed to create analysis proxy from '{source.name}'.\n"
            f"  Cause: {detail}\n"
            f"  Check: ...\n"
            f"  Tip: ..."
        )
        self.source = source
        self.detail = detail
```

<!-- Existing models from src/cinecut/models.py — used in test scaffold -->
```python
@dataclass
class KeyframeRecord:
    timestamp_s: float  # PTS seconds in the proxy
    frame_path: str     # Absolute path to the JPEG file
    source: str         # "subtitle_midpoint" | "scene_change" | "interval_fallback"
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add requests dependency and InferenceError/VramError classes</name>
  <files>
    pyproject.toml
    src/cinecut/errors.py
  </files>
  <action>
    1. In pyproject.toml, add `"requests>=2.31.0"` to the `dependencies` list. Keep alphabetical order within the list is fine; just append before the closing bracket.

    2. In src/cinecut/errors.py, append two new error classes after ConformError. Follow the exact pattern already established (CineCutError subclass, __init__ with actionable message, store args as attributes):

    ```python
    class InferenceError(CineCutError):
        def __init__(self, detail: str) -> None:
            super().__init__(
                f"LLaVA inference engine error.\n"
                f"  Cause: {detail}\n"
                f"  Check: Is llama-server installed at /usr/local/bin/llama-server? Is the LLaVA model at /home/adamh/models/ggml-model-q4_k.gguf?\n"
                f"  Tip: Run `llama-server --version` to verify the binary is accessible."
            )
            self.detail = detail

    class VramError(CineCutError):
        def __init__(self, detail: str) -> None:
            super().__init__(
                f"Insufficient VRAM for LLaVA inference.\n"
                f"  Cause: {detail}\n"
                f"  Check: Is another GPU process running? Run `nvidia-smi` to see current VRAM usage.\n"
                f"  Tip: Close other GPU applications and retry."
            )
            self.detail = detail
    ```

    Do NOT add `from pathlib import Path` — InferenceError and VramError do not take Path arguments. The existing `from pathlib import Path` import at the top of errors.py already covers the other classes.
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && pip install -e . -q && python3 -c "from cinecut.errors import InferenceError, VramError; e = InferenceError('test'); v = VramError('test'); print('OK')"</automated>
  </verify>
  <done>
    - pyproject.toml lists requests>=2.31.0 in dependencies
    - InferenceError and VramError importable from cinecut.errors
    - Both classes are CineCutError subclasses with actionable messages and detail attribute
  </done>
</task>

<task type="auto">
  <name>Task 2: Create test scaffold for Phase 3</name>
  <files>
    tests/test_inference.py
  </files>
  <action>
    Create tests/test_inference.py with a complete test scaffold. All tests must be structured so they PASS immediately (using mocks and skips where appropriate). The scaffold covers all 6 test cases from the RESEARCH.md validation architecture.

    The file structure:
    - Imports: pytest, unittest.mock, subprocess, threading, pathlib, json, base64, cinecut.errors
    - A module-level `LLAVA_MODEL_PATH` constant pointing to `/home/adamh/models/ggml-model-q4_k.gguf`
    - A module-level `MMPROJ_PATH` constant pointing to `/home/adamh/models/mmproj-model-f16.gguf`
    - A pytest mark: `integration = pytest.mark.skipif(not Path(LLAVA_MODEL_PATH).exists(), reason="LLaVA model not downloaded")`

    Tests to include:

    **test_server_health (integration mark):**
    Skip with integration mark. Test that LlavaEngine starts llama-server and /health returns {"status": "ok"}. Use `from cinecut.inference.engine import LlavaEngine` (will exist after plan 02). Body: `with LlavaEngine(Path(LLAVA_MODEL_PATH), Path(MMPROJ_PATH)) as engine: r = requests.get(f"{engine.base_url}/health", timeout=5); assert r.json()["status"] == "ok"`

    **test_no_model_reload (integration mark):**
    Skip with integration mark. Verify server PID is same for two consecutive describe_frame calls. Body placeholder: `pytest.skip("requires plan-02 implementation")`

    **test_describe_frame_structure (unit, mock server):**
    Mock `requests.post` to return a response with `choices[0].message.content` = valid JSON string matching SceneDescription schema. Assert that describe_frame returns a SceneDescription object with all four fields: visual_content, mood, action, setting. Use `from cinecut.inference.engine import LlavaEngine` and `from cinecut.inference.models import SceneDescription`. Mark with `pytest.mark.skip(reason="requires plan-02 implementation")` — this will be unskipped in plan 03.

    **test_malformed_response_skipped (unit, mock server):**
    Same skip. Mock requests.post to return invalid JSON. Assert describe_frame returns None.

    **test_vram_check (unit):**
    This test can run NOW. Mock `subprocess.run` to return stdout "500\n" (below VRAM_MINIMUM_MIB). Import `from cinecut.inference.vram import check_vram_free_mib, VRAM_MINIMUM_MIB`. Since vram.py doesn't exist yet, wrap entire test in try/except ImportError and skip: `pytest.importorskip("cinecut.inference.vram")`. Then assert VramError is raised when free VRAM < minimum.

    **test_gpu_lock (unit):**
    Same pytest.importorskip pattern for `cinecut.inference`. Assert GPU_LOCK is a threading.Lock instance: `from cinecut.inference import GPU_LOCK; assert isinstance(GPU_LOCK, type(threading.Lock()))`

    Use `pytest.importorskip` on the cinecut.inference imports so the scaffold file can be imported cleanly before plan 02 creates those modules.

    Full skeleton structure:
    ```python
    """Tests for Phase 3: LLaVA Inference Engine (INFR-01, INFR-02, INFR-03, PIPE-05)."""
    import base64
    import json
    import threading
    from pathlib import Path
    from unittest import mock

    import pytest
    import requests

    from cinecut.errors import InferenceError, VramError

    LLAVA_MODEL_PATH = "/home/adamh/models/ggml-model-q4_k.gguf"
    MMPROJ_PATH = "/home/adamh/models/mmproj-model-f16.gguf"
    _models_exist = Path(LLAVA_MODEL_PATH).exists() and Path(MMPROJ_PATH).exists()
    integration = pytest.mark.skipif(not _models_exist, reason="LLaVA model files not downloaded to /home/adamh/models/")

    # ... test functions ...
    ```
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python3 -m pytest tests/test_inference.py -x -q 2>&1 | tail -5</automated>
  </verify>
  <done>
    - tests/test_inference.py exists with all 6 test functions
    - File can be collected by pytest without ImportError
    - Tests that need plan-02 implementation are skipped (not failing)
    - test_vram_check and test_gpu_lock skip cleanly via pytest.importorskip
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Checkpoint: Verify setup and model download</name>
  <action>Human verification checkpoint — no automated implementation. See how-to-verify for verification steps.</action>
  <what-built>
    - requests declared in pyproject.toml
    - InferenceError and VramError added to src/cinecut/errors.py
    - tests/test_inference.py scaffold created and collectable
    - LLaVA model download instructions presented (user_setup above)
  </what-built>
  <how-to-verify>
    1. Verify error classes: `python3 -c "from cinecut.errors import InferenceError, VramError; print('OK')"` — should print OK

    2. Verify test scaffold: `python3 -m pytest tests/test_inference.py -v` — all tests should be skipped or pass, none should ERROR or FAIL

    3. Verify requests declared: `grep requests pyproject.toml` — should show requests>=2.31.0

    4. Download LLaVA model files (if not already done):
       ```
       mkdir -p /home/adamh/models
       wget -c "https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/ggml-model-q4_k.gguf" \
            -O /home/adamh/models/ggml-model-q4_k.gguf
       wget -c "https://huggingface.co/mys/ggml_llava-v1.5-7b/resolve/main/mmproj-model-f16.gguf" \
            -O /home/adamh/models/mmproj-model-f16.gguf
       ```
       Verify: `ls -lh /home/adamh/models/*.gguf` — both files should exist (~4+ GB for base, ~600 MB for mmproj)

    5. Once models downloaded, re-run: `python3 -m pytest tests/test_inference.py -v` — integration tests should no longer be skipped (they will now run and likely fail since plan-02 not built yet — that is expected)
  </how-to-verify>
  <resume-signal>Type "approved" when error classes verified, scaffold collectable, and model files downloaded (or in progress). Describe any issues.</resume-signal>
</task>

</tasks>

<verification>
- `python3 -c "from cinecut.errors import InferenceError, VramError; print('OK')"` prints OK
- `python3 -m pytest tests/test_inference.py -x -q` completes without ERROR or FAIL (skips are fine)
- `grep requests /home/adamh/ai-video-trailer/pyproject.toml` returns requests>=2.31.0
- `ls -lh /home/adamh/models/*.gguf` shows both model files
</verification>

<success_criteria>
- requests dependency declared in pyproject.toml
- InferenceError and VramError in cinecut.errors with actionable messages matching project error conventions
- tests/test_inference.py collectable by pytest with all 6 Phase 3 test cases scaffolded
- LLaVA GGUF and mmproj files on disk at /home/adamh/models/
- Human verified all of the above
</success_criteria>

<output>
After completion, create `.planning/phases/03-llava-inference-engine/03-01-SUMMARY.md`
</output>
