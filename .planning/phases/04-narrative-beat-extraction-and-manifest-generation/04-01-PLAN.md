---
phase: 04-narrative-beat-extraction-and-manifest-generation
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - src/cinecut/manifest/schema.py
  - src/cinecut/narrative/__init__.py
  - src/cinecut/narrative/signals.py
  - src/cinecut/narrative/scorer.py
autonomous: true
requirements:
  - NARR-02
  - NARR-03

must_haves:
  truths:
    - "ClipEntry accepts reasoning, visual_analysis, subtitle_analysis, money_shot_score as optional fields without breaking existing manifest tests"
    - "signals.py can extract all 8 raw signal values from a KeyframeRecord + SceneDescription + DialogueEvent list"
    - "scorer.py normalizes raw signal pools and computes a weighted money_shot_score in [0.0, 1.0]"
    - "scorer.py classifies each scene into one of exactly 7 beat types using chronological position, emotion, and score"
    - "scorer.py assigns an act value, always classifying beat type BEFORE act (breath beat forces act=breath)"
  artifacts:
    - path: "src/cinecut/manifest/schema.py"
      provides: "ClipEntry with 4 new Optional fields"
      contains: "reasoning: Optional[str] = None"
    - path: "src/cinecut/narrative/__init__.py"
      provides: "Package marker and public export surface"
    - path: "src/cinecut/narrative/signals.py"
      provides: "8-signal extraction from keyframe + metadata"
      exports: ["RawSignals", "extract_signals", "get_film_duration_s"]
    - path: "src/cinecut/narrative/scorer.py"
      provides: "Normalization, weighted scoring, beat classification, act assignment"
      exports: ["SIGNAL_WEIGHTS", "normalize_signal_pool", "compute_money_shot_score", "classify_beat", "assign_act"]
  key_links:
    - from: "src/cinecut/narrative/signals.py"
      to: "src/cinecut/models.py"
      via: "import KeyframeRecord, DialogueEvent"
      pattern: "from cinecut.models import"
    - from: "src/cinecut/narrative/signals.py"
      to: "src/cinecut/inference/models.py"
      via: "import SceneDescription"
      pattern: "from cinecut.inference.models import"
    - from: "src/cinecut/narrative/scorer.py"
      to: "src/cinecut/narrative/signals.py"
      via: "import RawSignals, normalize helpers"
      pattern: "from cinecut.narrative.signals import"
---

<objective>
Extend the ClipEntry schema with 4 analysis metadata fields and implement the two core narrative computation modules: signals.py (8-signal extraction) and scorer.py (normalization, weighted scoring, beat classification, act assignment).

Purpose: These modules are the algorithmic core of Phase 4. They transform raw (KeyframeRecord, SceneDescription | None, DialogueEvent list) inputs into money_shot_score floats and beat_type/act strings — the data that generator.py (Plan 02) will use to assemble TRAILER_MANIFEST.json.

Output: Extended schema.py, narrative/__init__.py, narrative/signals.py, narrative/scorer.py.
</objective>

<execution_context>
@/home/adamh/.claude/get-shit-done/workflows/execute-plan.md
@/home/adamh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

<interfaces>
<!-- Key types the executor needs. No codebase exploration required. -->

From src/cinecut/models.py:
```python
@dataclass
class DialogueEvent:
    start_ms: int
    end_ms: int
    start_s: float
    end_s: float
    midpoint_s: float
    text: str
    emotion: str  # "positive" | "negative" | "neutral" | "intense" | "comedic" | "romantic"

@dataclass
class KeyframeRecord:
    timestamp_s: float  # PTS seconds in the proxy
    frame_path: str     # Absolute path to the JPEG file
    source: str         # "subtitle_midpoint" | "scene_change" | "interval_fallback"
```

From src/cinecut/inference/models.py:
```python
@dataclass
class SceneDescription:
    visual_content: str
    mood: str
    action: str
    setting: str
```

From src/cinecut/manifest/schema.py (current ClipEntry — to be extended):
```python
class ClipEntry(BaseModel):
    source_start_s: float = Field(ge=0.0)
    source_end_s: float = Field(ge=0.0)
    beat_type: Literal[
        "inciting_incident", "character_introduction", "escalation_beat",
        "relationship_beat", "money_shot", "climax_peak", "breath",
    ]
    act: Literal[
        "cold_open", "act1", "beat_drop", "act2", "breath", "act3", "title_card", "button",
    ]
    transition: Literal["hard_cut", "crossfade", "fade_to_black", "fade_to_white"] = "hard_cut"
    dialogue_excerpt: str = ""

    @model_validator(mode="after")
    def end_after_start(self) -> "ClipEntry": ...
```

OpenCV package on this machine: `opencv-python-headless 4.13.0.92`
- All cv2 APIs verified working. Import: `import cv2`
- Haar cascade path: `cv2.data.haarcascades + "haarcascade_frontalface_default.xml"`
- cv2.data IS available in opencv-python-headless
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend ClipEntry with 4 Optional analysis fields</name>
  <files>src/cinecut/manifest/schema.py</files>
  <action>
Add `from typing import Optional` to imports. Add 4 Optional fields to ClipEntry AFTER the existing `dialogue_excerpt` field and BEFORE the `end_after_start` validator:

```python
# Phase 4 additions (EDIT-01): analysis metadata
reasoning: Optional[str] = None
visual_analysis: Optional[str] = None
subtitle_analysis: Optional[str] = None
money_shot_score: Optional[float] = Field(default=None, ge=0.0, le=1.0)
```

ALL four fields must be Optional with None defaults so existing hand-crafted manifests in tests/fixtures/ load without modification. Do not change any other part of schema.py — TrailerManifest, VALID_VIBES, and the existing ClipEntry fields are unchanged.

After editing, run `python3 -m pytest tests/test_manifest.py -x -q` to confirm existing manifest tests still pass. The schema extension must be backward compatible.
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python3 -m pytest tests/test_manifest.py -x -q 2>&1 | tail -5</automated>
  </verify>
  <done>tests/test_manifest.py passes without modification; ClipEntry has reasoning, visual_analysis, subtitle_analysis, money_shot_score as Optional fields; python3 -c "from cinecut.manifest.schema import ClipEntry; c = ClipEntry(source_start_s=1.0, source_end_s=5.0, beat_type='breath', act='breath'); assert c.money_shot_score is None" exits 0</done>
</task>

<task type="auto">
  <name>Task 2: Create narrative package — signals.py and scorer.py</name>
  <files>
    src/cinecut/narrative/__init__.py
    src/cinecut/narrative/signals.py
    src/cinecut/narrative/scorer.py
  </files>
  <action>
Create the `src/cinecut/narrative/` directory and three files.

**src/cinecut/narrative/__init__.py**
Empty module marker (just a docstring):
```python
"""Narrative beat extraction, money shot scoring, and manifest generation."""
```

**src/cinecut/narrative/signals.py**
Implements 8-signal extraction. Key implementation notes:

1. Use a `@dataclass` named `RawSignals` with fields: `motion_magnitude: float`, `visual_contrast: float`, `scene_uniqueness: float` (placeholder — filled in by pool computation), `subtitle_emotional_weight: float`, `face_presence: float`, `llava_confidence: float`, `saturation: float`, `chronological_position: float`. Also include `_histogram: object` (non-field, set after construction for pool uniqueness).

2. Load `cv2.CascadeClassifier` ONCE at module level (not per-frame) to avoid 200ms per-frame penalty:
```python
import cv2
_FACE_CASCADE = cv2.CascadeClassifier(
    cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
)
```

3. `EMOTION_WEIGHTS: dict[str, float]` mapping: `"intense": 1.0, "romantic": 0.7, "negative": 0.6, "comedic": 0.5, "positive": 0.4, "neutral": 0.1`.

4. `get_film_duration_s(source_file: Path) -> float`: Run ffprobe with `-v quiet -print_format json -show_format`, parse JSON, return `float(data["format"]["duration"])`. Raises `subprocess.CalledProcessError` on failure (let it propagate — CLI catches CineCutError, this will surface as unhandled and is acceptable for an unexpected ffprobe failure).

5. `get_subtitle_emotional_weight(timestamp_s: float, dialogue_events: list[DialogueEvent], window_s: float = 5.0) -> float`: Find nearest dialogue event within window_s. If timestamp_s falls within event.start_s..event.end_s, return `EMOTION_WEIGHTS.get(event.emotion, 0.0)` immediately. Otherwise find nearest event by min distance to start/end; return its weight if within window_s, else 0.0.

6. `compute_llava_confidence(desc: SceneDescription | None) -> float`: If None, return 0.0. Score completeness of 4 fields (visual_content, mood, action, setting) at 0.5 weight + richness (min(1.0, total_char_len / 200.0)) at 0.5 weight.

7. `extract_image_signals(frame_path: str) -> dict`: Load with `cv2.imread(frame_path)`. If None (corrupt/missing), return dict with all image signals at 0.0 and `_histogram` as None. Compute: `visual_contrast = float(cv2.Laplacian(gray, cv2.CV_64F).var())`, `saturation = float(hsv[:, :, 1].mean())`, `face_presence = 1.0 if len(_FACE_CASCADE.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30,30))) > 0 else 0.0`, and a normalized HSV histogram `cv2.calcHist([hsv], [0,1], None, [50,60], [0,180,0,256])` stored as `_histogram`.

8. `compute_motion_magnitudes(frame_paths: list[str]) -> list[float]`: Load all frames as grayscale floats. First frame gets 0.0. Each subsequent frame: `float(np.abs(gray - prev).mean())`. If frame is unreadable (None), append 0.0 and keep previous gray unchanged.

9. `compute_uniqueness_scores(histograms: list) -> list[float]`: O(n²) pairwise `cv2.compareHist(h, other, cv2.HISTCMP_CORREL)`. If histogram is None, uniqueness = 0.5. Uniqueness = `max(0.0, 1.0 - max(0.0, max_sim_to_others))`. For N < 2, return [0.5] * N.

10. `extract_all_signals(records: list[KeyframeRecord], dialogue_events: list[DialogueEvent], film_duration_s: float) -> list[RawSignals]`: The main entry point.
    - Build `frame_paths = [r.frame_path for r in records]`
    - Call `compute_motion_magnitudes(frame_paths)` → motion list
    - For each record, call `extract_image_signals(record.frame_path)` → collect all per-frame dicts
    - Call `compute_uniqueness_scores([d["_histogram"] for d in img_data])` → uniqueness list
    - For each record i, build `RawSignals(motion_magnitude=motions[i], visual_contrast=img_data[i]["visual_contrast"], scene_uniqueness=uniqueness[i], subtitle_emotional_weight=get_subtitle_emotional_weight(record.timestamp_s, dialogue_events), face_presence=img_data[i]["face_presence"], llava_confidence=compute_llava_confidence(desc_for_record), saturation=img_data[i]["saturation"], chronological_position=min(1.0, record.timestamp_s / film_duration_s))`
    - The caller must pass `desc_for_record` — so signature: `extract_all_signals(records, scene_descriptions: list[SceneDescription | None], dialogue_events, film_duration_s) -> list[RawSignals]`

**src/cinecut/narrative/scorer.py**
Implements normalization, weighted sum, beat classification, and act assignment.

1. `SIGNAL_WEIGHTS: dict[str, float]` — weights must sum to exactly 1.0:
```python
SIGNAL_WEIGHTS = {
    "motion_magnitude":          0.20,
    "visual_contrast":           0.15,
    "scene_uniqueness":          0.15,
    "subtitle_emotional_weight": 0.20,
    "face_presence":             0.10,
    "llava_confidence":          0.10,
    "saturation":                0.05,
    "chronological_position":    0.05,
}
assert abs(sum(SIGNAL_WEIGHTS.values()) - 1.0) < 1e-9
```

2. `normalize_signal_pool(raw_values: list[float]) -> list[float]`: Min-max normalization across the pool. If `max == min`, return `[0.5] * len(raw_values)`.

3. `normalize_all_signals(raw_signals: list[RawSignals]) -> list[dict[str, float]]`: For each of the 8 signal names, collect the raw value across all records, normalize the pool, then rebuild per-record normalized dicts. Returns a list of `dict[str, float]` where each dict has exactly the 8 SIGNAL_WEIGHTS keys.

4. `compute_money_shot_score(normalized: dict[str, float]) -> float`: `sum(SIGNAL_WEIGHTS[k] * normalized[k] for k in SIGNAL_WEIGHTS)`.

5. `classify_beat(chron_pos: float, emotion: str, money_shot_score: float, has_face: bool) -> str`: Rule-based priority order (earlier rule wins):
   - `money_shot_score < 0.20 and emotion == "neutral"` → `"breath"`
   - `chron_pos > 0.75 and money_shot_score > 0.70` → `"climax_peak"`
   - `money_shot_score > 0.80` → `"money_shot"`
   - `chron_pos < 0.15 and has_face and emotion not in ("intense",)` → `"character_introduction"`
   - `chron_pos < 0.30 and emotion == "intense"` → `"inciting_incident"`
   - `emotion == "romantic" and has_face` → `"relationship_beat"`
   - catch-all → `"escalation_beat"`

6. `assign_act(chron_pos: float, beat_type: str) -> str`: Beat type wins first: `if beat_type == "breath": return "breath"`. Then position: `< 0.08 → "cold_open"`, `< 0.35 → "act1"`, `< 0.55 → "act2"`, `< 0.65 → "beat_drop"`, `< 0.82 → "act2"`, else `"act3"`.

Add `"opencv-contrib-python-headless>=4.8.0"` to pyproject.toml dependencies (the package already installed is `opencv-python-headless` — use this exact declaration since that is what pip shows). Actually: `pip list` shows `opencv-python-headless 4.13.0.92`. Declare as `"opencv-python-headless>=4.8.0"` in pyproject.toml.
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python3 -c "
from cinecut.narrative.signals import extract_all_signals, get_film_duration_s, EMOTION_WEIGHTS
from cinecut.narrative.scorer import SIGNAL_WEIGHTS, classify_beat, assign_act, normalize_signal_pool
# Test signal weights sum
assert abs(sum(SIGNAL_WEIGHTS.values()) - 1.0) < 1e-9, 'weights do not sum to 1.0'
# Test beat classification
assert classify_beat(0.9, 'intense', 0.85, True) == 'climax_peak'
assert classify_beat(0.5, 'neutral', 0.15, False) == 'breath'
assert classify_beat(0.5, 'intense', 0.90, False) == 'money_shot'
assert classify_beat(0.05, 'positive', 0.50, True) == 'character_introduction'
# Test act assignment - beat type wins
assert assign_act(0.9, 'breath') == 'breath'
assert assign_act(0.05, 'escalation_beat') == 'cold_open'
# Test normalization degenerate case
assert normalize_signal_pool([5.0, 5.0, 5.0]) == [0.5, 0.5, 0.5]
print('ALL ASSERTIONS PASSED')
" 2>&1</automated>
  </verify>
  <done>All assertion checks pass; signals.py and scorer.py importable; SIGNAL_WEIGHTS sum to 1.0; beat classification and act assignment functions return correct values for all test cases; existing test_manifest.py still passes after pyproject.toml update</done>
</task>

</tasks>

<verification>
Run full test suite to confirm no regressions from schema extension:

```bash
cd /home/adamh/ai-video-trailer && python3 -m pytest tests/ -x -q --ignore=tests/test_inference.py 2>&1 | tail -10
```

Confirm narrative package is importable:
```bash
cd /home/adamh/ai-video-trailer && python3 -c "from cinecut.narrative.signals import extract_all_signals; from cinecut.narrative.scorer import classify_beat, compute_money_shot_score; print('import OK')"
```
</verification>

<success_criteria>
- ClipEntry has 4 new Optional fields; all existing manifest tests pass (backward compatible)
- `cinecut.narrative.signals` and `cinecut.narrative.scorer` importable
- SIGNAL_WEIGHTS sum to 1.0 exactly
- classify_beat returns one of the 7 valid beat_type literals for all valid inputs
- assign_act returns "breath" when beat_type is "breath" regardless of chron_pos
- opencv-python-headless declared in pyproject.toml
</success_criteria>

<output>
After completion, create `.planning/phases/04-narrative-beat-extraction-and-manifest-generation/04-01-SUMMARY.md`
</output>
