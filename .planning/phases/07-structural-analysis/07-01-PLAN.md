---
phase: 07-structural-analysis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/cinecut/inference/text_engine.py
  - src/cinecut/inference/vram.py
  - src/cinecut/inference/__init__.py
  - src/cinecut/cli.py
autonomous: true
requirements: [IINF-03]

must_haves:
  truths:
    - "TextEngine starts llama-server on port 8090 (never port 8089) and terminates it on context-manager exit"
    - "TextEngine acquires GPU_LOCK on __enter__ and releases it on __exit__ — LlavaEngine and TextEngine cannot run concurrently"
    - "Before acquiring GPU_LOCK, TextEngine polls VRAM at 2s intervals until 6144 MiB is free or 60s elapses"
    - "Setting CINECUT_MODELS_DIR=/custom/path causes all three model paths (LLaVA, mmproj, Mistral) to resolve from /custom/path"
    - "cli.py no longer contains hard-coded /home/adamh/models paths — all model paths use get_models_dir()"
  artifacts:
    - path: "src/cinecut/inference/text_engine.py"
      provides: "TextEngine context manager, get_models_dir(), MISTRAL_GGUF_NAME constant"
      exports: ["TextEngine", "get_models_dir", "MISTRAL_GGUF_NAME"]
    - path: "src/cinecut/inference/vram.py"
      provides: "wait_for_vram() polling function (added alongside existing check_vram_free_mib)"
      contains: "def wait_for_vram"
    - path: "src/cinecut/cli.py"
      provides: "get_models_dir() used for all three model paths; _DEFAULT_MODEL_PATH removed"
      contains: "get_models_dir()"
  key_links:
    - from: "src/cinecut/inference/text_engine.py TextEngine.__enter__"
      to: "src/cinecut/inference/vram.py wait_for_vram"
      via: "call before GPU_LOCK.acquire()"
      pattern: "wait_for_vram\\("
    - from: "src/cinecut/cli.py"
      to: "src/cinecut/inference/text_engine.py get_models_dir"
      via: "import and call at runtime inside main()"
      pattern: "get_models_dir\\(\\)"
---

<objective>
Create the TextEngine context manager (inference/text_engine.py) — a text-only llama-server wrapper mirroring LlavaEngine — and migrate all model path resolution to CINECUT_MODELS_DIR.

Purpose: TextEngine is the GPU-locked inference host for Mistral 7B. It must serialize correctly with LlavaEngine (same GPU_LOCK), use VRAM polling between model swaps, and live on port 8090. The CINECUT_MODELS_DIR migration (IINF-03) affects three paths in cli.py and must happen in this plan so Plan 07-02 can use get_models_dir() when wiring Stage 5.

Output:
- src/cinecut/inference/text_engine.py (new)
- src/cinecut/inference/vram.py (extended: wait_for_vram)
- src/cinecut/inference/__init__.py (export TextEngine)
- src/cinecut/cli.py (model paths migrated to get_models_dir)
</objective>

<execution_context>
@/home/adamh/.claude/get-shit-done/workflows/execute-plan.md
@/home/adamh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-inference-persistence/06-01-SUMMARY.md

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/cinecut/inference/engine.py (LlavaEngine — the pattern to mirror):
```python
class LlavaEngine:
    def __init__(self, model_path: Path, mmproj_path: Path, port: int = 8089, debug: bool = False) -> None
    def __enter__(self) -> "LlavaEngine"   # acquires GPU_LOCK, calls _start()
    def __exit__(self, *_: object) -> None  # calls _stop(), releases GPU_LOCK
    def _start(self) -> None               # launches llama-server, calls _wait_for_health(timeout_s=120)
    def _wait_for_health(self, timeout_s: float) -> None  # polls /health every 1s
    def _stop(self) -> None                # SIGTERM → SIGKILL on timeout=10s

    # llama-server cmd for LLaVA (has --mmproj):
    # ["llama-server", "-m", model_path, "--mmproj", mmproj_path, "--port", port,
    #  "--host", "127.0.0.1", "-ngl", "99", "-c", "2048", "-np", "1", "--log-disable"]
    # CRITICAL: stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL (never PIPE)
```

From src/cinecut/inference/__init__.py:
```python
GPU_LOCK: threading.Lock = threading.Lock()
# Exports: GPU_LOCK, LlavaEngine, SceneDescription, SCENE_DESCRIPTION_SCHEMA,
#          check_vram_free_mib, VRAM_MINIMUM_MIB
```

From src/cinecut/inference/vram.py:
```python
VRAM_MINIMUM_MIB: int = 6144  # 6 GB
def check_vram_free_mib() -> int   # runs nvidia-smi; raises VramError if < VRAM_MINIMUM_MIB
def assert_vram_available() -> None  # calls check_vram_free_mib(), raises VramError
```

From src/cinecut/errors.py (inferred — use existing error types):
```python
class VramError(CineCutError): ...
class InferenceError(CineCutError): ...
```

From src/cinecut/cli.py (current hard-coded paths to replace):
```python
_DEFAULT_MODEL_PATH = "/home/adamh/models/ggml-model-q4_k.gguf"
_DEFAULT_MMPROJ_PATH = "/home/adamh/models/mmproj-model-f16.gguf"
TOTAL_STAGES = 7
# model/mmproj CLI args use Path(_DEFAULT_MODEL_PATH) as default=
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create inference/text_engine.py with TextEngine and get_models_dir</name>
  <files>src/cinecut/inference/text_engine.py</files>
  <action>
Create src/cinecut/inference/text_engine.py with the following:

1. Module-level constant:
   ```python
   MISTRAL_GGUF_NAME = "mistral-7b-instruct-v0.3.Q4_K_M.gguf"
   ```

2. `get_models_dir() -> Path` function:
   - `env_val = os.environ.get("CINECUT_MODELS_DIR")`
   - If set: `return Path(env_val).expanduser().resolve()`
   - Default: `return Path.home() / "models"`
   - Call `expanduser()` then `resolve()` — handles `~` in env value
   - Do NOT use `os.path.expandvars()` — the env var IS the path

3. `TextEngine` class — context manager mirroring LlavaEngine exactly with these three differences:
   - Port 8090 (not 8089), default `port: int = 8090`
   - No `mmproj_path` parameter — text-only model
   - `-c 8192` in the llama-server cmd (not `-c 2048`)
   - No `--mmproj` flag in cmd

   The `__enter__` method must:
   - Call `wait_for_vram()` from vram.py BEFORE `GPU_LOCK.acquire()` — not `assert_vram_available()`
   - Acquire `GPU_LOCK` (from `cinecut.inference import GPU_LOCK`)
   - Call `self._start()` inside try/except; release `GPU_LOCK` on exception

   The `_start()` method cmd must be:
   ```python
   cmd = [
       "llama-server",
       "-m", str(self.model_path),
       "--port", str(self.port),
       "--host", "127.0.0.1",
       "-ngl", "99",
       "-c", "8192",
       "-np", "1",
       "--log-disable",
       # NO --mmproj
   ]
   self._process = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
   ```

   Copy `_wait_for_health` and `_stop` verbatim from LlavaEngine (same logic applies).

   Add `analyze_chunk(self, chunk_text: str, timeout_s: float = 60.0) -> dict | None` method — this is consumed by structural.py in Plan 07-02. Full implementation in Plan 07-02; add the method stub here that returns `None` (will be replaced in 07-02). This allows 07-02 to complete the method without the structural.py import chain failing.

   Actually: do NOT stub analyze_chunk here. Plan 07-02 adds it directly to text_engine.py as part of its task 1. Leave the class with only context-manager methods.

4. Imports needed: `import json, os, subprocess, time; from pathlib import Path; import requests; from cinecut.errors import InferenceError, VramError; from cinecut.inference.vram import wait_for_vram`
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python -c "from cinecut.inference.text_engine import TextEngine, get_models_dir, MISTRAL_GGUF_NAME; print('OK:', MISTRAL_GGUF_NAME)"</automated>
  </verify>
  <done>Module imports cleanly; TextEngine class exists; get_models_dir() returns Path.home()/"models" when CINECUT_MODELS_DIR is unset; MISTRAL_GGUF_NAME = "mistral-7b-instruct-v0.3.Q4_K_M.gguf"</done>
</task>

<task type="auto">
  <name>Task 2: Add wait_for_vram to vram.py and export TextEngine from inference/__init__.py</name>
  <files>
    src/cinecut/inference/vram.py
    src/cinecut/inference/__init__.py
  </files>
  <action>
**vram.py — add wait_for_vram:**

Add a non-raising version of VRAM query and a polling loop. Add these two functions after the existing `assert_vram_available`:

```python
def _check_vram_free_mib_raw() -> int:
    """Query free VRAM without raising VramError. Returns 0 on any failure."""
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=memory.free", "--format=csv,noheader,nounits"],
            capture_output=True, text=True, check=True, timeout=10,
        )
        return int(result.stdout.strip().splitlines()[0])
    except Exception:
        return 0


def wait_for_vram(
    min_free_mib: int = VRAM_MINIMUM_MIB,
    poll_interval_s: float = 2.0,
    timeout_s: float = 60.0,
) -> None:
    """Poll nvidia-smi until at least min_free_mib MiB is free.

    Called between LlavaEngine exit and TextEngine entry to wait for VRAM
    release after llama-server process termination (OS reclaims pages async).
    Raises VramError if VRAM does not free within timeout_s.
    """
    import time
    deadline = time.monotonic() + timeout_s
    while time.monotonic() < deadline:
        free_mib = _check_vram_free_mib_raw()
        if free_mib >= min_free_mib:
            return
        time.sleep(poll_interval_s)
    raise VramError(
        f"VRAM did not reach {min_free_mib} MiB free within {timeout_s}s after model swap"
    )
```

**inference/__init__.py — export TextEngine:**

Add to the existing __init__.py (after the GPU_LOCK line):
```python
from cinecut.inference.text_engine import TextEngine, get_models_dir, MISTRAL_GGUF_NAME  # noqa: E402
```

Add `"TextEngine"`, `"get_models_dir"`, `"MISTRAL_GGUF_NAME"`, `"wait_for_vram"` to `__all__`.

Also add vram import:
```python
from cinecut.inference.vram import check_vram_free_mib, VRAM_MINIMUM_MIB, wait_for_vram  # noqa: E402
```
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python -c "from cinecut.inference import TextEngine, get_models_dir, wait_for_vram; print('OK')"</automated>
  </verify>
  <done>wait_for_vram importable from cinecut.inference.vram; TextEngine importable from cinecut.inference; __all__ updated in __init__.py</done>
</task>

<task type="auto">
  <name>Task 3: Migrate cli.py model paths to get_models_dir (IINF-03)</name>
  <files>src/cinecut/cli.py</files>
  <action>
In cli.py, replace the hard-coded model path constants and CLI defaults with `get_models_dir()` resolution.

**Step 1 — Add import:**
Add to the existing imports at top of cli.py:
```python
from cinecut.inference.text_engine import get_models_dir, MISTRAL_GGUF_NAME
```

**Step 2 — Remove hard-coded constants:**
Delete these two lines entirely:
```python
_DEFAULT_MODEL_PATH = "/home/adamh/models/ggml-model-q4_k.gguf"
_DEFAULT_MMPROJ_PATH = "/home/adamh/models/mmproj-model-f16.gguf"
```

**Step 3 — Update CLI argument defaults:**
The `model` and `mmproj` typer options currently use `default=Path(_DEFAULT_MODEL_PATH)` and `default=Path(_DEFAULT_MMPROJ_PATH)`. These must NOT use a hardcoded Path as the default — `get_models_dir()` must be called at runtime.

Change both to `default=None` and resolve them inside `main()`:
```python
model: Annotated[
    Optional[Path],
    typer.Option("--model", file_okay=True, dir_okay=False, resolve_path=True,
                 help="Path to LLaVA GGUF model (default: CINECUT_MODELS_DIR/ggml-model-q4_k.gguf)."),
] = None,
mmproj: Annotated[
    Optional[Path],
    typer.Option("--mmproj", file_okay=True, dir_okay=False, resolve_path=True,
                 help="Path to mmproj GGUF (default: CINECUT_MODELS_DIR/mmproj-model-f16.gguf)."),
] = None,
```

**Step 4 — Resolve model paths at runtime inside main():**
After the `work_dir` setup line and before Stage 1, add:
```python
# Resolve model paths at runtime — respects CINECUT_MODELS_DIR (IINF-03)
models_dir = get_models_dir()
if model is None:
    model = models_dir / "ggml-model-q4_k.gguf"
if mmproj is None:
    mmproj = models_dir / "mmproj-model-f16.gguf"
```

**Step 5 — Update help string references:**
The `--model` and `--mmproj` help strings previously referenced `_DEFAULT_MODEL_PATH` f-strings — update them to static strings as shown in Step 3.

Do NOT change TOTAL_STAGES here — that is done in Plan 07-02 when Stage 5 is inserted.

The existing `run_inference_stage(keyframe_records, model, mmproj, ...)` call continues to work unchanged since `model` and `mmproj` are still Path objects by the time they are used.
  </action>
  <verify>
    <automated>cd /home/adamh/ai-video-trailer && python -c "import cinecut.cli; print('OK')" && CINECUT_MODELS_DIR=/tmp python -c "from cinecut.inference.text_engine import get_models_dir; p = get_models_dir(); assert str(p) == '/tmp', f'Expected /tmp got {p}'; print('CINECUT_MODELS_DIR override OK')"</automated>
  </verify>
  <done>cli.py imports without error; no _DEFAULT_MODEL_PATH or _DEFAULT_MMPROJ_PATH in source; CINECUT_MODELS_DIR=/tmp causes get_models_dir() to return Path("/tmp"); existing tests pass: pytest tests/ -x -q</done>
</task>

</tasks>

<verification>
Run full test suite to ensure no regressions from the cli.py model path migration:

```
cd /home/adamh/ai-video-trailer && pytest tests/ -x -q
```

Verify CINECUT_MODELS_DIR override works end-to-end:
```
CINECUT_MODELS_DIR=/custom/path python -c "
from cinecut.inference.text_engine import get_models_dir, MISTRAL_GGUF_NAME
d = get_models_dir()
assert str(d) == '/custom/path', d
print('CINECUT_MODELS_DIR:', d)
print('Mistral GGUF:', d / MISTRAL_GGUF_NAME)
"
```

Verify TextEngine class structure (no llama-server actually started):
```
python -c "
import inspect
from cinecut.inference.text_engine import TextEngine
src = inspect.getsource(TextEngine._start)
assert '--mmproj' not in src, 'TextEngine must not have --mmproj'
assert '8090' in src or '8192' in src, 'Port 8090 and context 8192 must be in _start'
print('TextEngine _start OK')
"
```
</verification>

<success_criteria>
- src/cinecut/inference/text_engine.py exists with TextEngine, get_models_dir, MISTRAL_GGUF_NAME
- wait_for_vram() added to vram.py; importable from cinecut.inference
- TextEngine uses port 8090, -c 8192, no --mmproj in cmd
- cli.py: no hard-coded /home/adamh/models paths; model paths resolved via get_models_dir() at runtime
- CINECUT_MODELS_DIR=/custom/path overrides all three model paths in cli.py
- All existing tests pass: pytest tests/ -x -q
</success_criteria>

<output>
After completion, create `.planning/phases/07-structural-analysis/07-01-SUMMARY.md`
</output>
